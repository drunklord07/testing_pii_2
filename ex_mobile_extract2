#!/usr/bin/env python3
import os
import re
import sys
import json
import uuid
import argparse
import traceback
import threading
import time
import random
import multiprocessing as mp
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from collections import defaultdict
from tqdm import tqdm

# ============= DEFAULT CONFIG ============= #
INPUT_FOLDER    = "input_logs"              # put your input folder name here (in current dir)
OUTPUT_FOLDER   = "output_mobile_regex"     # outputs will be created under this
MAX_WORKERS     = 6
CHUNK_SIZE      = 10_000                    # rotate output files every 10k lines
MIRROR_TRUNCATE = 1500                      # first N chars of log_line kept (used for BOTH extracted & mirror)
SUMMARY_REFRESH_INTERVAL = 30               # seconds (live summary rewrite)
RESUME_LOG      = Path(OUTPUT_FOLDER) / "resume_files.log"   # tracks successfully processed files

# JSON nested key-path inference (hardcoded per your request)
NESTED_JSON_DETECT   = True
FIELD_PATH_MODE      = "full"               # "last" or "full"
JSON_BACKSCAN_WINDOW = 800

# Stats shard knobs
EXAMPLE_MAXLEN  = 500                       # limit example string length in stats shards
PATH_ONLY_MAX   = 5                         # per-file cap for path-only sample lines
# ========================================== #

# Derived paths
OUT_FIELDS_DIR     = Path(OUTPUT_FOLDER) / "fields_identified"
OUT_MIRROR_DIR     = Path(OUTPUT_FOLDER) / "mirror"
OUT_FIELDS_SHARDS  = OUT_FIELDS_DIR / "shards"
OUT_MIRROR_SHARDS  = OUT_MIRROR_DIR / "shards"
STATS_SHARDS_DIR   = Path(OUTPUT_FOLDER) / "_stats_shards"
SUMMARY_FILE       = Path(OUTPUT_FOLDER) / "summary_mobile_fields.txt"
ERRORS_FILE        = Path(OUTPUT_FOLDER) / "errors.log"

# Strict mobile regex (India 10 or prefixed 91 + 10); prevents A–Z / 0–9 adjacency
MOBILE_RE = re.compile(r'(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])')

# ---------- field detection patterns ----------
def make_patterns(value_escaped: str):
    p_json_quoted = re.compile(
        rf'["\']\s*(?P<field>[^"\']+?)\s*["\']\s*[:=]\s*["\']?(?P<value>{value_escaped})["\']?',
        re.IGNORECASE,
    )
    p_kv = re.compile(
        rf'\b(?P<field>[A-Za-z0-9_.\-]+)\s*[:=]\s*["\']?(?P<value>{value_escaped})["\']?',
        re.IGNORECASE,
    )
    p_xml_attr = re.compile(
        rf'<[^>]*\b(?P<field>[A-Za-z0-9_.\-]+)\s*=\s*["\'](?P<value>{value_escaped})["\'][^>]*>',
        re.IGNORECASE | re.DOTALL,
    )
    p_xml_tag = re.compile(
        rf'<\s*(?P<field>[A-Za-z0-9_.\-]+)[^>]*>[^<]*?(?P<value>{value_escaped})[^<]*?</\s*\1\s*>',
        re.IGNORECASE | re.DOTALL,
    )
    p_bracketed = re.compile(
        rf'\[\s*(?P<field>[A-Za-z0-9_.\-]+)\s*[:=]\s*(?P<value>{value_escaped})\s*\]',
        re.IGNORECASE,
    )
    p_curly = re.compile(
        rf'\{{[^}}]*?(?P<field>[A-Za-z0-9_.\-]+)\s*[:=]\s*(?P<value>{value_escaped})(?:[^}}]*?)\}}',
        re.IGNORECASE,
    )
    p_xml_inline = re.compile(
        rf'<[^>]*?\bname\s*=\s*["\']?(?P<field>[A-Za-z0-9_.\-]+)["\']?[^>]*>'
        rf'[^<]*?(?P<value>{value_escaped})[^<]*?</[^>]+>',
        re.IGNORECASE | re.DOTALL,
    )
    return (p_json_quoted, p_kv, p_xml_attr, p_xml_tag, p_bracketed, p_curly, p_xml_inline)

# ---------- nested JSON helpers ----------
def _quote_aware_scan_levels(s: str):
    lvl = [0] * (len(s) + 1)
    depth = 0
    in_str = False
    esc = False
    for i, ch in enumerate(s):
        if in_str:
            if esc:
                esc = False
            elif ch == '\\':
                esc = True
            elif ch == '"':
                in_str = False
        else:
            if ch == '"':
                in_str = True
            elif ch in '{[':
                depth += 1
            elif ch in '}]' and depth > 0:
                depth -= 1
        lvl[i+1] = depth
    return lvl

_key_colon_re = re.compile(r'"(?P<k>[^"\\]+)"\s*:')

def _nearest_json_key_for_value(s: str, val_start: int, window: int = 400, path_mode: str = "last"):
    if val_start <= 0:
        return None
    L = max(0, val_start - window)
    slice_s = s[L:val_start]
    lvl = _quote_aware_scan_levels(slice_s)

    candidates = []
    for m in _key_colon_re.finditer(slice_s):
        k = m.group('k').strip()
        key_depth = lvl[m.start()]
        candidates.append((m.start(), m.end(), key_depth, k))
    if not candidates:
        return None

    best_key = None
    best_key_depth = -1
    for start_idx, after_colon, key_depth, key in reversed(candidates):
        cut = False
        base_depth = lvl[start_idx]
        for i in range(after_colon, len(slice_s)):
            if slice_s[i] == ',' and lvl[i] == base_depth and (L + i) < val_start:
                cut = True; break
            if slice_s[i] in '}]' and lvl[i+1] < base_depth and (L + i) < val_start:
                cut = True; break
        if not cut:
            best_key = key
            best_key_depth = key_depth
            break

    if best_key is None:
        return None

    if path_mode == "last":
        return best_key

    # dotted path from nearest upwards
    path = [best_key]
    target_depth = best_key_depth
    for start_idx, after_colon, key_depth, key in reversed(candidates):
        if key_depth < target_depth:
            path.append(key)
            target_depth = key_depth
    path.reverse()
    return ".".join(path)

def identify_field_for_value(log_line: str, value: str):
    val_esc = re.escape(value)
    for pat in make_patterns(val_esc):
        m = pat.search(log_line)
        if m:
            field = m.group("field").strip()
            if field:
                return field
    if NESTED_JSON_DETECT:
        idx = log_line.find(value)
        if idx != -1:
            key_or_path = _nearest_json_key_for_value(
                log_line, idx, JSON_BACKSCAN_WINDOW, FIELD_PATH_MODE
            )
            if key_or_path:
                return key_or_path
    return None

# ---------------- per-worker shard writer ----------------
class ShardWriter:
    """
    Each worker writes directly to shard files (no IPC for data).
    Files rotate after CHUNK_SIZE lines.
    """
    def __init__(self, base_dir: Path, prefix: str, pid: int, chunk_size: int):
        self.base_dir = base_dir
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.prefix = prefix
        self.pid = pid
        self.chunk_size = chunk_size
        self.counter = 1
        self.lines = 0
        self.current_path = self.base_dir / f"{self.prefix}_pid{self.pid}_{self.counter:04d}.txt"
        self.current = self.current_path.open("w", encoding="utf-8")
        self.produced = []

    def write(self, s: str):
        if not s:
            return
        try:
            self.current.write(s)
        except OSError as e:
            if getattr(e, "errno", None) == 28:
                raise
            else:
                raise
        self.lines += s.count("\n")
        if self.lines >= self.chunk_size:
            self._roll()

    def _roll(self):
        self.current.close()
        if self.current_path.stat().st_size > 0:
            self.produced.append(str(self.current_path))
        self.counter += 1
        self.lines = 0
        self.current_path = self.base_dir / f"{self.prefix}_pid{self.pid}_{self.counter:04d}.txt"
        self.current = self.current_path.open("w", encoding="utf-8")

    def close(self):
        self.current.close()
        if self.current_path.stat().st_size > 0:
            self.produced.append(str(self.current_path))
        return self.produced

# ---------------- stats shard helpers ----------------
def _safe_trunc(s: str, n: int) -> str:
    return s if len(s) <= n else (s[:n] + "...TRUNCATED...")

def write_stats_shard(run_id: str,
                      input_path: Path,
                      stats: dict,
                      per_field_counts: dict,
                      per_field_example: dict,
                      path_only_samples: list):
    """
    Write one JSONL record per input file. Keeps counts complete; trims example/sample text for safety.
    """
    STATS_SHARDS_DIR.mkdir(parents=True, exist_ok=True)
    rec = {
        "run_id": run_id,
        "input_path": str(input_path),
        "timestamp": datetime.now().isoformat(timespec='seconds'),
        "stats": stats,  # totals for this file (dedup-aware)
        "per_field_counts": per_field_counts,
        "per_field_example": {k: _safe_trunc(v, EXAMPLE_MAXLEN) for k, v in per_field_example.items()},
        "path_only_samples": [_safe_trunc(x, EXAMPLE_MAXLEN) for x in path_only_samples[:PATH_ONLY_MAX]],
    }
    fname = f"stats_{run_id}_{os.getpid()}_{uuid.uuid4().hex}.jsonl"
    with (STATS_SHARDS_DIR / fname).open("a", encoding="utf-8") as fh:
        fh.write(json.dumps(rec, ensure_ascii=False) + "\n")

# ---------------- worker ----------------
def process_file(path: Path, run_id: str):
    """
    Workers write directly to shard files and emit a small stats shard.
    Returns: (file_failed: bool, path_str: str, disk_full: bool)
    """
    stats = defaultdict(int)
    per_field_counts = defaultdict(int)
    per_field_example = {}
    path_only_samples = []
    file_failed = False
    disk_full = False

    pid = os.getpid()
    fields_writer = ShardWriter(OUT_FIELDS_SHARDS, "extracted", pid, CHUNK_SIZE)
    mirror_writer = ShardWriter(OUT_MIRROR_SHARDS, "mirror", pid, CHUNK_SIZE)

    try:
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            for raw in f:
                line = raw.rstrip("\n")
                if not line:
                    continue

                # split "log_line ; file_path"
                if ";" in line:
                    log_line, file_path = line.rsplit(";", 1)
                else:
                    log_line, file_path = line, ""

                matches = list(MOBILE_RE.finditer(line))
                if not matches:
                    stats["lines_no_regex"] += 1
                    continue

                split_at = len(log_line)
                log_matches  = [m for m in matches if m.start() < split_at]
                path_matches = [m for m in matches if m.start() >= split_at]

                stats["total_regex_matches"] += len(matches)

                if (not log_matches) and path_matches:
                    stats["dropped_path_only_matches"] += len(path_matches)
                    if len(path_only_samples) < PATH_ONLY_MAX:
                        path_only_samples.append(line)
                    continue

                # Per-line de-dup by field
                field_first_value = {}
                unidentified_present = False

                for m in log_matches:
                    mobile_val = m.group(0)
                    field = identify_field_for_value(log_line, mobile_val)
                    if field:
                        key_norm = field.strip().lower()
                        if key_norm not in field_first_value:
                            field_first_value[key_norm] = (field, mobile_val)
                    else:
                        unidentified_present = True

                if not field_first_value and not unidentified_present:
                    continue

                short_log = log_line if len(log_line) <= MIRROR_TRUNCATE else (log_line[:MIRROR_TRUNCATE] + "...TRUNCATED...")

                # Emit one extracted row per unique field
                had_extr = False
                for _, (field, mob) in field_first_value.items():
                    row = f"{short_log} ; {file_path} ; {field} ; mobile_regex ; {mob}\n"
                    fields_writer.write(row)
                    stats["extracted_rows"] += 1
                    per_field_counts[field] += 1
                    if field not in per_field_example:
                        per_field_example[field] = row.strip()
                    had_extr = True

                # Emit at most one mirror row if any unidentified occurred
                had_mirr = False
                if unidentified_present:
                    first_mob = log_matches[0].group(0)
                    row = f"{short_log} ; {file_path} ; UNIDENTIFIED_FIELD ; mobile_regex ; {first_mob}\n"
                    mirror_writer.write(row)
                    stats["mirror_rows"] += 1
                    had_mirr = True

                if had_extr and had_mirr:
                    stats["partial_valid_lines"] += 1

    except OSError as e:
        file_failed = True
        if getattr(e, "errno", None) == 28:
            disk_full = True
        stats["errors"] += 1
        try:
            with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
                ef.write(f"[{datetime.now().isoformat(timespec='seconds')}] {path}: {e}\n")
                ef.write(traceback.format_exc() + "\n")
        except Exception:
            pass
    except Exception as e:
        file_failed = True
        stats["errors"] += 1
        try:
            with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
                ef.write(f"[{datetime.now().isoformat(timespec='seconds')}] {path}: {e}\n")
                ef.write(traceback.format_exc() + "\n")
        except Exception:
            pass

    # Close writers & emit stats shard
    try:
        fields_writer.close()
        mirror_writer.close()
    except OSError as e:
        if getattr(e, "errno", None) == 28:
            disk_full = True
        file_failed = True
        stats["errors"] += 1
        try:
            with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
                ef.write(f"[{datetime.now().isoformat(timespec='seconds')}] writer_close {path}: {e}\n")
                ef.write(traceback.format_exc() + "\n")
        except Exception:
            pass

    try:
        write_stats_shard(run_id, path, dict(stats), dict(per_field_counts), per_field_example, path_only_samples)
    except Exception as e:
        try:
            with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
                ef.write(f"[{datetime.now().isoformat(timespec='seconds')}] stats_shard_write {path}: {e}\n")
                ef.write(traceback.format_exc() + "\n")
        except Exception:
            pass

    return (file_failed, str(path), disk_full)

# ---------------- reduce stats shards (RUN_ID aware) ----------------
def reduce_stats_shards(run_id: str):
    """
    Aggregate all per-file JSONL stats shards for this run_id into global counters, per-field counts, examples, and samples.
    Returns: (G_stats, G_field_counts, G_field_example, G_path_only_samples)
    """
    G_stats = defaultdict(int)
    G_field_counts = defaultdict(int)
    G_field_example = {}
    G_path_only_samples = []

    if not STATS_SHARDS_DIR.exists():
        return G_stats, G_field_counts, G_field_example, G_path_only_samples

    shards = sorted([p for p in STATS_SHARDS_DIR.glob(f"stats_{run_id}_*.jsonl") if p.is_file()])
    for shard in shards:
        try:
            with shard.open("r", encoding="utf-8", errors="ignore") as fh:
                for line in fh:
                    if not line.strip():
                        continue
                    rec = json.loads(line)
                    if rec.get("run_id") != run_id:
                        continue
                    stats = rec.get("stats", {})
                    for k, v in stats.items():
                        try:
                            G_stats[k] += int(v)
                        except Exception:
                            pass

                    pfc = rec.get("per_field_counts", {})
                    for fld, cnt in pfc.items():
                        try:
                            G_field_counts[fld] += int(cnt)
                        except Exception:
                            pass

                    pfe = rec.get("per_field_example", {})
                    for fld, ex in pfe.items():
                        if fld not in G_field_example and ex:
                            G_field_example[fld] = ex

                    pos = rec.get("path_only_samples", [])
                    for s in pos:
                        if len(G_path_only_samples) < 50:
                            G_path_only_samples.append(s)
        except Exception as e:
            try:
                with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
                    ef.write(f"[{datetime.now().isoformat(timespec='seconds')}] reduce_stats_shards {shard}: {e}\n")
                    ef.write(traceback.format_exc() + "\n")
            except Exception:
                pass

    return G_stats, G_field_counts, G_field_example, G_path_only_samples

# ---------------- summary writer (dedup-aware) ----------------
def write_summary(input_dir: Path,
                  run_id: str,
                  G_stats: dict,
                  G_field_counts: dict,
                  G_field_example: dict,
                  G_path_only_samples: list,
                  failed_files: list,
                  stage: str = "Final",
                  files_total: int = 0,
                  files_processed: int = 0,
                  files_failed: int = 0):
    raw_total     = G_stats.get("total_regex_matches", 0)
    dropped_path  = G_stats.get("dropped_path_only_matches", 0)
    lines_no_regex= G_stats.get("lines_no_regex", 0)
    extr_rows     = G_stats.get("extracted_rows", 0)  # dedup output rows
    mirr_rows     = G_stats.get("mirror_rows", 0)     # dedup output rows
    partial_lines = G_stats.get("partial_valid_lines", 0)
    errors_count  = G_stats.get("errors", 0)

    with open(SUMMARY_FILE, "w", encoding="utf-8") as sf:
        sf.write(f"Summary ({stage}) - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | RUN_ID={run_id}\n")
        sf.write("=" * 60 + "\n\n")

        sf.write("INPUT / OUTPUT\n")
        sf.write(f"Input folder: {input_dir.resolve()}\n")
        sf.write(f"Output folder: {Path(OUTPUT_FOLDER).resolve()}\n")
        sf.write(f"Fields Identified (chunk size {CHUNK_SIZE}): {OUT_FIELDS_DIR.resolve()}\n")
        sf.write(f"Mirror (chunk size {CHUNK_SIZE}): {OUT_MIRROR_DIR.resolve()}\n")
        sf.write(f"Truncation: first {MIRROR_TRUNCATE} chars of log_line (applies to extracted & mirror)\n\n")

        sf.write("FILE COUNTS\n")
        sf.write(f"Total files discovered: {files_total}\n")
        sf.write(f"Files processed successfully: {files_processed}\n")
        sf.write(f"Files failed: {files_failed}\n\n")

        if failed_files:
            sf.write("FAILED FILES\n")
            for f in failed_files[:100]:
                sf.write(f"- {f}\n")
            sf.write("\n")

        sf.write("COUNTS (raw vs output rows)\n")
        sf.write(f"Raw regex matches (log+path): {raw_total}\n")
        sf.write(f"  Dropped (path-only matches): {dropped_path}\n")
        sf.write(f"  Lines with no regex (dropped lines): {lines_no_regex}\n")
        sf.write(f"Output rows (dedup):\n")
        sf.write(f"  Extracted rows: {extr_rows}\n")
        sf.write(f"  Mirror rows:    {mirr_rows}\n")
        sf.write(f"Partial-valid lines: {partial_lines}\n")
        sf.write(f"Errors: {errors_count}\n\n")

        sf.write("PER-FIELD COUNTS (extracted)\n")
        for fld, cnt in sorted(G_field_counts.items(), key=lambda kv: kv[0].lower()):
            sf.write(f"{fld} = {cnt}\n")
            ex = G_field_example.get(fld)
            if ex:
                sf.write(f"  Example: {ex}\n")
        sf.write("\n")

        sf.write("SAMPLE PATH-ONLY LINES\n")
        if G_path_only_samples:
            for i, ln in enumerate(G_path_only_samples[:50], 1):
                sf.write(f"{i}. {ln}\n")
        else:
            sf.write("(none)\n")
        sf.write("\n")

        sf.write("NOTES\n")
        sf.write("- Per-line de-dup by field: at most one extracted row per unique field per line.\n")
        sf.write("- One mirror row per line if any unidentified field present.\n")
        sf.write("- Nested-JSON key-path inference enabled; returns dotted paths.\n")
        sf.write("- Summary counts are for this RUN_ID only.\n")

# ---------------- resume helpers ----------------
def load_resume_set() -> set:
    if RESUME_LOG.exists():
        try:
            return set(p.strip() for p in RESUME_LOG.read_text(encoding="utf-8", errors="ignore").splitlines() if p.strip())
        except Exception:
            return set()
    return set()

def append_resume(path_str: str):
    try:
        RESUME_LOG.parent.mkdir(parents=True, exist_ok=True)
        with open(RESUME_LOG, "a", encoding="utf-8") as f:
            f.write(path_str + "\n")
    except Exception:
        pass

# ---------------- live summary refresher ----------------
def summary_refresher_loop(input_dir: Path,
                           run_id: str,
                           files_total_getter,
                           files_progress_getter,
                           failed_files_getter,
                           stop_event: threading.Event):
    while not stop_event.is_set():
        try:
            G_stats, G_field_counts, G_field_example, G_path_only_samples = reduce_stats_shards(run_id)
            files_total, files_processed, files_failed = files_total_getter(), *files_progress_getter()
            failed_files = failed_files_getter()
            write_summary(input_dir, run_id, G_stats, G_field_counts, G_field_example,
                          G_path_only_samples, failed_files, stage="Live",
                          files_total=files_total, files_processed=files_processed, files_failed=files_failed)
        except Exception as e:
            try:
                with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
                    ef.write(f"[{datetime.now().isoformat(timespec='seconds')}] summary_refresher: {e}\n")
                    ef.write(traceback.format_exc() + "\n")
            except Exception:
                pass
        stop_event.wait(SUMMARY_REFRESH_INTERVAL)

# ---------------- merge shards (direct, verified) ----------------
def merge_shards_and_verify(run_id: str) -> bool:
    """
    Merge shards directly into final files (10k lines each), then verify counts against stats reducer.
    Only delete shards if counts match. Return True on success (deleted), False if kept.
    """
    def _merge_one(shard_dir: Path, out_dir: Path, prefix: str, expected_lines: int) -> int:
        out_dir.mkdir(parents=True, exist_ok=True)
        shards = sorted([p for p in shard_dir.glob(f"{prefix}_pid*_*.txt") if p.is_file()])
        total_written = 0
        if not shards:
            return 0
        file_index = 1
        lines_written = 0
        out_fh = (out_dir / f"{prefix}_{file_index:03d}.txt").open("w", encoding="utf-8")
        try:
            for shard in shards:
                with shard.open("r", encoding="utf-8", errors="ignore") as sf:
                    for line in sf:
                        out_fh.write(line)
                        lines_written += 1
                        total_written += 1
                        if lines_written >= CHUNK_SIZE:
                            out_fh.close()
                            file_index += 1
                            lines_written = 0
                            out_fh = (out_dir / f"{prefix}_{file_index:03d}.txt").open("w", encoding="utf-8")
            out_fh.close()
        finally:
            try:
                if not out_fh.closed:
                    out_fh.close()
            except Exception:
                pass
        return total_written

    # Reduce expected counts first
    G_stats, *_ = reduce_stats_shards(run_id)
    expected_extracted = int(G_stats.get("extracted_rows", 0))
    expected_mirror    = int(G_stats.get("mirror_rows", 0))

    # Merge directly
    wrote_extracted = _merge_one(OUT_FIELDS_SHARDS, OUT_FIELDS_DIR, "extracted", expected_extracted)
    wrote_mirror    = _merge_one(OUT_MIRROR_SHARDS, OUT_MIRROR_DIR, "mirror", expected_mirror)

    # Verify
    ok = (wrote_extracted == expected_extracted) and (wrote_mirror == expected_mirror)

    # Delete shards only if counts match
    if ok:
        for d in (OUT_FIELDS_SHARDS, OUT_MIRROR_SHARDS):
            if d.exists():
                for p in list(d.glob("*")):
                    try: p.unlink()
                    except Exception: pass
                try:
                    next(d.iterdir())
                except StopIteration:
                    try: d.rmdir()
                    except Exception: pass
    else:
        # Keep shards for merge-only retry; log a warning
        try:
            with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
                ef.write(f"[{datetime.now().isoformat(timespec='seconds')}] merge_verify_mismatch: "
                         f"expected extracted={expected_extracted}, mirror={expected_mirror}; "
                         f"wrote extracted={wrote_extracted}, mirror={wrote_mirror}\n")
        except Exception:
            pass

    return ok

# ---------------- CLI / main ----------------
def parse_args():
    ap = argparse.ArgumentParser(description="Mobile extractor with dedup + verified merge")
    ap.add_argument("--merge-only", action="store_true",
                    help="Only merge shards and write summary (no processing)")
    ap.add_argument("--keep-stats", action="store_true",
                    help="Do not clean _stats_shards at start")
    ap.add_argument("--run-id", type=str, default=None,
                    help="Override RUN_ID (advanced). Default: generate new UUID on processing; "
                         "in --merge-only, you must pass the correct RUN_ID for the shards you want to merge.")
    return ap.parse_args()

def main():
    args = parse_args()

    input_dir = Path(INPUT_FOLDER)
    if not input_dir.exists():
        print(f"Input folder not found: {input_dir.resolve()}")
        sys.exit(1)

    # Prepare dirs
    Path(OUTPUT_FOLDER).mkdir(parents=True, exist_ok=True)
    OUT_FIELDS_DIR.mkdir(parents=True, exist_ok=True)
    OUT_MIRROR_DIR.mkdir(parents=True, exist_ok=True)
    OUT_FIELDS_SHARDS.mkdir(parents=True, exist_ok=True)
    OUT_MIRROR_SHARDS.mkdir(parents=True, exist_ok=True)
    STATS_SHARDS_DIR.mkdir(parents=True, exist_ok=True)

    # RUN_ID handling
    if args.merge_only:
        if not args.run_id:
            print("--merge-only requires --run-id matching the shards to merge.")
            sys.exit(1)
        run_id = args.run_id
    else:
        run_id = args.run_id or uuid.uuid4().hex

    # Clean stats shards at start unless told not to (only for processing runs)
    if not args.merge_only and not args.keep_stats:
        # remove any old stats_* files to prevent cross-run contamination
        try:
            for p in STATS_SHARDS_DIR.glob("stats_*.jsonl"):
                try: p.unlink()
                except Exception: pass
        except Exception:
            pass

    # Discover files
    files = [p for p in input_dir.rglob("*.txt")]
    if not files and not args.merge_only:
        print("No .txt files found in input.")
        G_stats, G_field_counts, G_field_example, G_path_only_samples = reduce_stats_shards(run_id)
        write_summary(input_dir, run_id, G_stats, G_field_counts, G_field_example, G_path_only_samples, [],
                      stage="Final", files_total=0, files_processed=0, files_failed=0)
        sys.exit(0)

    if args.merge_only:
        ok = merge_shards_and_verify(run_id)
        G_stats, G_field_counts, G_field_example, G_path_only_samples = reduce_stats_shards(run_id)
        write_summary(input_dir, run_id, G_stats, G_field_counts, G_field_example,
                      G_path_only_samples, [], stage=("Final" if ok else "Merge Mismatch"),
                      files_total=len(files), files_processed=len(files), files_failed=0)
        print("\n✅ Merge-only complete." if ok else "\n⚠️  Merge-only finished with mismatch; shards preserved.")
        print(f"Summary: {SUMMARY_FILE.resolve()}")
        if not ok:
            print("Check errors.log for details. You can re-run --merge-only after investigating.")
        return

    already = load_resume_set()
    pending = [p for p in files if str(p) not in already]
    random.shuffle(pending)

    files_total = len(files)
    files_processed = len(already)
    failed_files = []
    disk_full_detected = False

    # Live summary refresher
    stop_event = threading.Event()
    refresher = threading.Thread(
        target=summary_refresher_loop,
        args=(
            input_dir,
            run_id,
            lambda: files_total,
            lambda: (files_processed, len(failed_files)),
            lambda: failed_files,
            stop_event,
        ),
        daemon=True
    )
    refresher.start()

    ctx = mp.get_context("spawn")

    try:
        with ProcessPoolExecutor(max_workers=MAX_WORKERS, mp_context=ctx) as ex:
            futures = {ex.submit(process_file, p, run_id): p for p in pending}
            for fut in tqdm(as_completed(futures), total=len(futures), desc="Processing"):
                path = futures[fut]
                try:
                    file_failed, path_str, disk_full = fut.result()
                    if disk_full:
                        disk_full_detected = True
                    if file_failed:
                        failed_files.append(path_str)
                    else:
                        files_processed += 1
                        append_resume(path_str)
                    if disk_full_detected:
                        break
                except KeyboardInterrupt:
                    raise
                except Exception as e:
                    failed_files.append(str(path))
                    try:
                        with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
                            ef.write(f"[{datetime.now().isoformat(timespec='seconds')}] {path}: {e}\n")
                            ef.write(traceback.format_exc() + "\n")
                    except Exception:
                        pass
    except KeyboardInterrupt:
        print("\n^C detected — shutting down gracefully...")
    finally:
        # Stop refresher
        stop_event.set()
        refresher.join(timeout=2)

        if disk_full_detected:
            # Do NOT merge; write aborted summary and exit.
            G_stats, G_field_counts, G_field_example, G_path_only_samples = reduce_stats_shards(run_id)
            write_summary(input_dir, run_id, G_stats, G_field_counts, G_field_example,
                          G_path_only_samples, failed_files, stage="Aborted (Disk Full)",
                          files_total=files_total, files_processed=files_processed, files_failed=len(failed_files))
            print("\n❌ Aborted: No space left on device. Free disk space and re-run to resume.")
            print(f"Summary:            {SUMMARY_FILE.resolve()}")
            print(f"Errors logged:      {ERRORS_FILE.resolve()}")
            sys.exit(2)

        # Merge shards directly to final files and verify counts
        ok = merge_shards_and_verify(run_id)

        # Final reduce for summary
        G_stats, G_field_counts, G_field_example, G_path_only_samples = reduce_stats_shards(run_id)
        stage = "Final" if ok else "Merge Mismatch (Shards Kept)"
        write_summary(input_dir, run_id, G_stats, G_field_counts, G_field_example,
                      G_path_only_samples, failed_files, stage=stage,
                      files_total=files_total, files_processed=files_processed, files_failed=len(failed_files))

        if ok:
            print("\n✅ Done.")
        else:
            print("\n⚠️  Done with merge verification MISMATCH. Shards preserved for retry (--merge-only --run-id YOUR_RUN_ID).")
        print(f"RUN_ID:             {run_id}")
        print(f"Total files: {files_total}  |  Processed: {files_processed}  |  Failed: {len(failed_files)}")
        print(f"Extracted files in: {OUT_FIELDS_DIR.resolve()}")
        print(f"Mirror files in:    {OUT_MIRROR_DIR.resolve()}")
        print(f"Summary:            {SUMMARY_FILE.resolve()}")
        if failed_files or not ok:
            print(f"Errors logged:      {ERRORS_FILE.resolve()}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print("Fatal error:", e)
        print(traceback.format_exc())
