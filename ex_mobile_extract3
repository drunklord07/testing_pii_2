
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, re, sys, traceback, random, multiprocessing as mp, threading
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

# ============= CONFIG ============= #
INPUT_FOLDER    = "input_logs"
OUTPUT_FOLDER   = "output_mobile_regex"
CHUNK_SIZE      = 10_000
MIRROR_TRUNCATE = 1500
SUMMARY_REFRESH_INTERVAL = 30
MAX_WORKERS     = 6
# ================================== #

OUT_FIELDS_DIR  = Path(OUTPUT_FOLDER) / "fields_identified"
OUT_MIRROR_DIR  = Path(OUTPUT_FOLDER) / "mirror"
SUMMARY_FILE    = Path(OUTPUT_FOLDER) / "summary_mobile_fields.txt"
ERRORS_FILE     = Path(OUTPUT_FOLDER) / "errors.log"
RESUME_LOG      = Path(OUTPUT_FOLDER) / "resume_files.log"

# Mobile regex (strict India 10-digit or 91+10-digit, not embedded)
MOBILE_RE = re.compile(r'(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])')

# ---------- field detection patterns ----------
def make_patterns(mobile_escaped: str):
    return (
        re.compile(
            rf'["\']\s*(?P<field>[^"\']+?)\s*["\']\s*[:=]\s*["\']?(?P<mobile>{mobile_escaped})["\']?',
            re.IGNORECASE,
        ),
        re.compile(
            rf'\b(?P<field>[A-Za-z0-9_.\-]+)\s*[:=]\s*["\']?(?P<mobile>{mobile_escaped})["\']?',
            re.IGNORECASE,
        ),
        re.compile(
            rf'<[^>]*\b(?P<field>[A-Za-z0-9_.\-]+)\s*=\s*["\'](?P<mobile>{mobile_escaped})["\'][^>]*>',
            re.IGNORECASE | re.DOTALL,
        ),
        re.compile(
            rf'<\s*(?P<field>[A-Za-z0-9_.\-]+)[^>]*>[^<]*?(?P<mobile>{mobile_escaped})[^<]*?</\s*\1\s*>',
            re.IGNORECASE | re.DOTALL,
        ),
        re.compile(   # bracketed key style
            rf'\[(?P<field>[A-Za-z0-9_.\-]+)\s*[:=]\s*(?P<mobile>{mobile_escaped})\]',
            re.IGNORECASE,
        ),
    )

def identify_field_for_mobile(log_line: str, mobile: str):
    mob_esc = re.escape(mobile)
    for pat in make_patterns(mob_esc):
        m = pat.search(log_line)
        if m:
            fld = m.group("field").strip()
            if fld:
                return fld
    return None

# ---------- Writer with chunk rotation ----------
class ChunkWriter:
    def __init__(self, base_dir: Path, prefix: str, chunk_size: int):
        self.base_dir = base_dir
        self.prefix = prefix
        self.chunk_size = chunk_size
        self.counter = mp.Value('i', 1)   # shared int
        self.lines_in_current = mp.Value('i', 0)
        self.lock = mp.Lock()

    def write(self, text: str):
        with self.lock:
            fn = self.base_dir / f"{self.prefix}_{self.counter.value:03d}.txt"
            mode = "a" if fn.exists() else "w"
            with fn.open(mode, encoding="utf-8") as f:
                f.write(text)
            self.lines_in_current.value += 1
            if self.lines_in_current.value >= self.chunk_size:
                self.counter.value += 1
                self.lines_in_current.value = 0

# ---------- Worker ----------
def process_file(path: Path, fields_writer: ChunkWriter, mirror_writer: ChunkWriter):
    stats = defaultdict(int)
    per_field_counts = defaultdict(int)
    per_field_example = {}
    file_failed = False

    try:
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            for raw in f:
                line = raw.rstrip("\n")
                if not line:
                    continue

                if ";" in line:
                    log_line, file_path = line.rsplit(";", 1)
                else:
                    log_line, file_path = line, ""

                matches = list(MOBILE_RE.finditer(log_line))
                if not matches:
                    stats["lines_no_regex"] += 1
                    continue
                stats["total_regex_matches"] += len(matches)

                # per-line dedup by field
                seen_fields = set()
                line_had_extracted, line_had_mirror = False, False

                for m in matches:
                    mobile_val = m.group(0)
                    field = identify_field_for_mobile(log_line, mobile_val)
                    if field:
                        if field not in seen_fields:
                            row = f"{log_line[:MIRROR_TRUNCATE]} ; {file_path} ; {field} ; mobile_regex ; {mobile_val}\n"
                            fields_writer.write(row)
                            stats["extracted_matches"] += 1
                            per_field_counts[field] += 1
                            if field not in per_field_example:
                                per_field_example[field] = row.strip()
                            seen_fields.add(field)
                            line_had_extracted = True
                    else:
                        if not line_had_mirror:
                            short_log = (
                                log_line[:MIRROR_TRUNCATE] + "...TRUNCATED..."
                                if len(log_line) > MIRROR_TRUNCATE else log_line
                            )
                            row = f"{short_log} ; {file_path} ; UNIDENTIFIED_FIELD ; mobile_regex ; {mobile_val}\n"
                            mirror_writer.write(row)
                            stats["mirrored_matches"] += 1
                            line_had_mirror = True

                if line_had_extracted and line_had_mirror:
                    stats["partial_valid_lines"] += 1

    except Exception as e:
        stats["errors"] += 1
        file_failed = True
        with open(ERRORS_FILE, "a", encoding="utf-8") as ef:
            ef.write(f"[{datetime.now().isoformat()}] {path}: {e}\n")
            ef.write(traceback.format_exc() + "\n")

    return stats, per_field_counts, per_field_example, file_failed

# ---------- Summary ----------
def write_summary(input_dir, G_stats, G_field_counts, G_field_example, failed_files, stage="Final"):
    with open(SUMMARY_FILE, "w", encoding="utf-8") as sf:
        sf.write(f"Summary ({stage}) - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        sf.write("="*60 + "\n\n")
        sf.write("INPUT / OUTPUT\n")
        sf.write(f"Input folder: {input_dir.resolve()}\n")
        sf.write(f"Output folder: {Path(OUTPUT_FOLDER).resolve()}\n")
        sf.write(f"Fields Identified: {OUT_FIELDS_DIR.resolve()}\n")
        sf.write(f"Mirror: {OUT_MIRROR_DIR.resolve()}\n\n")
        sf.write("FILE COUNTS\n")
        sf.write(f"Total files discovered: {G_stats['files_total']}\n")
        sf.write(f"Files processed successfully: {G_stats['files_processed']}\n")
        sf.write(f"Files failed: {G_stats['files_failed']}\n\n")
        sf.write("COUNTS (raw vs dedup logic)\n")
        sf.write(f"Raw regex matches: {G_stats.get('total_regex_matches',0)}\n")
        sf.write(f"Extracted rows: {G_stats.get('extracted_matches',0)}\n")
        sf.write(f"Mirror rows: {G_stats.get('mirrored_matches',0)}\n")
        sf.write(f"Partial-valid lines: {G_stats.get('partial_valid_lines',0)}\n")
        sf.write(f"Lines with no regex: {G_stats.get('lines_no_regex',0)}\n")
        sf.write(f"Errors: {G_stats.get('errors',0)}\n\n")
        sf.write("PER-FIELD COUNTS\n")
        for fld, cnt in sorted(G_field_counts.items(), key=lambda kv: kv[0].lower()):
            sf.write(f"{fld} = {cnt}\n")
            ex = G_field_example.get(fld)
            if ex:
                sf.write(f"  Example: {ex}\n")

def summary_refresher(input_dir, G_stats, G_field_counts, G_field_example, failed_files, stop_event):
    while not stop_event.is_set():
        try:
            write_summary(input_dir, G_stats, G_field_counts, G_field_example, failed_files, stage="Live")
        except Exception as e:
            with open(ERRORS_FILE,"a",encoding="utf-8") as ef:
                ef.write(f"[{datetime.now().isoformat()}] refresher: {e}\n")
                ef.write(traceback.format_exc()+"\n")
        stop_event.wait(SUMMARY_REFRESH_INTERVAL)

# ---------- Main ----------
def main():
    input_dir = Path(INPUT_FOLDER)
    if not input_dir.exists():
        print(f"Input folder not found: {input_dir.resolve()}")
        sys.exit(1)

    OUT_FIELDS_DIR.mkdir(parents=True, exist_ok=True)
    OUT_MIRROR_DIR.mkdir(parents=True, exist_ok=True)

    files = [p for p in input_dir.rglob("*.txt")]
    if not files:
        print("No .txt files found.")
        sys.exit(0)
    random.shuffle(files)

    fields_writer = ChunkWriter(OUT_FIELDS_DIR, "extracted", CHUNK_SIZE)
    mirror_writer = ChunkWriter(OUT_MIRROR_DIR, "mirror", CHUNK_SIZE)

    G_stats = defaultdict(int)
    G_field_counts = defaultdict(int)
    G_field_example = {}
    failed_files = []
    G_stats["files_total"] = len(files)

    stop_event = threading.Event()
    refresher = threading.Thread(
        target=summary_refresher, 
        args=(input_dir,G_stats,G_field_counts,G_field_example,failed_files,stop_event),
        daemon=True
    )
    refresher.start()

    with ProcessPoolExecutor(max_workers=MAX_WORKERS, mp_context=mp.get_context("spawn")) as ex:
        futures = {ex.submit(process_file,p,fields_writer,mirror_writer): p for p in files}
        for fut in tqdm(as_completed(futures), total=len(futures), desc="Processing"):
            path = futures[fut]
            try:
                stats, field_counts, field_example, file_failed = fut.result()
                if file_failed:
                    G_stats["files_failed"] += 1
                    failed_files.append(str(path))
                else:
                    G_stats["files_processed"] += 1
                for k,v in stats.items():
                    G_stats[k]+=v
                for k,v in field_counts.items():
                    G_field_counts[k]+=v
                for k,v in field_example.items():
                    if k not in G_field_example:
                        G_field_example[k]=v
            except Exception as e:
                G_stats["files_failed"]+=1
                failed_files.append(str(path))
                with open(ERRORS_FILE,"a",encoding="utf-8") as ef:
                    ef.write(f"[{datetime.now().isoformat()}] {path}: {e}\n")
                    ef.write(traceback.format_exc()+"\n")

    stop_event.set()
    refresher.join(timeout=2)
    write_summary(input_dir,G_stats,G_field_counts,G_field_example,failed_files,"Final")

    print("\n✅ Done.")
    print(f"Total files: {G_stats['files_total']} | Processed: {G_stats['files_processed']} | Failed: {G_stats['files_failed']}")
    print(f"Extracted files in: {OUT_FIELDS_DIR.resolve()}")
    print(f"Mirror files in:    {OUT_MIRROR_DIR.resolve()}")
    print(f"Summary:            {SUMMARY_FILE.resolve()}")
    if G_stats['files_failed']>0:
        print(f"Errors logged:      {ERRORS_FILE.resolve()}")

if __name__=="__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n⚠️ Interrupted by user (Ctrl+C).")
        sys.exit(1)
    except Exception as e:
        print("Fatal error:", e)
        print(traceback.format_exc())

