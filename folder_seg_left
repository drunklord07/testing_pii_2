#!/usr/bin/env python3
import re
import traceback
from pathlib import Path
from functools import lru_cache
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed
from threading import Thread
import multiprocessing as mp
from tqdm import tqdm

# ============= CONFIG ============= #
INPUT_FOLDER     = "input_logs"          # put your input folder name here (in current dir)
OUTPUT_FOLDER    = "output_pii"          # outputs will be created under this
SUMMARY_FILE     = "summary_report.txt"  # summary file in current dir
MAX_WORKERS      = 8                     # number of worker PROCESSES
LINES_PER_FILE   = 10000                 # split size per PII type file
BATCH_SIZE       = 1000                  # queue batch size from workers to writer

# Limit outputs to specific types (folder names). Empty set() => include ALL.
ONLY_REGEX_TYPES   = {"upi", "pan", "voterid"}  # lower-case; must match REGEX_FOLDER values
ONLY_KEYWORD_TYPES = set()                      # e.g. {"name","address"}; empty disables keyword handling
# ================================== #

# ---------- REGEX PATTERNS ----------
PII_PATTERNS = {
    "MOBILE_REGEX": re.compile(r'(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])'),
    "AADHAAR_REGEX": re.compile(r'(?<![A-Za-z0-9])(\d{12})(?![A-Za-z0-9])'),
    "PAN_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{5}\d{4}[A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "GSTIN_REGEX": re.compile(r'(?<![A-Za-z0-9])\d{2}[A-Z]{5}\d{4}[A-Z][1-9A-Z]Z[0-9A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "DL_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{2}\d{2}\d{11}(?![A-Za-z0-9])', re.IGNORECASE),
    "VOTERID_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{3}\d{7}(?![A-Za-z0-9])', re.IGNORECASE),

    # EMAIL: TLD >=2
    "EMAIL_REGEX": re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', re.IGNORECASE),

    # UPI: allow quotes after PSP
    "UPI_REGEX": re.compile(
        r'[A-Za-z0-9._-]{2,256}@[A-Za-z]{2,64}(?=$|[\"\'\s<>\[\]\{\}\(\),;_\-])'
    ),

    # IPv4 strict 0–255
    "IP_REGEX": re.compile(
        r'(?<!\d)'
        r'(?:(?:25[0-5]|2[0-4]\d|1?\d?\d)\.){3}'
        r'(?:25[0-5]|2[0-4]\d|1?\d?\d)'
        r'(?!\d)'
    ),

    "MAC_REGEX": re.compile(r'(?<![A-Fa-f0-9])(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}(?![A-Fa-f0-9])'),

    # Card numbers with alnum lookarounds; validated with Luhn.
    "CARD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])('
        r'4\d{12}(?:\d{3})?'
        r'|5[1-5]\d{14}'
        r'|2(?:2[2-9]\d{12}|[3-6]\d{13}|7(?:[01]\d{12}|20\d{12}))'
        r'|3[47]\d{13}'
        r'|60\d{14}|65\d{14}|81\d{14}|508\d\d{12}'
        r')(?![A-Za-z0-9])'
    ),

    # Coords: decimals required for both lat/lon
    "COORD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])'
        r'([+-]?(?:90\.(?:0+)?|[0-8]?\d\.\d+))'
        r'\s*,\s*'
        r'([+-]?(?:180\.(?:0+)?|1[0-7]\d\.\d+|[0-9]?\d\.\d+))'
        r'(?![A-Za-z0-9])'
    ),
}

# ---------- Aadhaar Verhoeff ----------
@lru_cache(maxsize=10000)
def is_valid_aadhaar(number: str) -> bool:
    if len(number) != 12 or not number.isdigit():
        return False
    mul = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,2,3,4,0,6,7,8,9,5],
        [2,3,4,0,1,7,8,9,5,6],
        [3,4,0,1,2,8,9,5,6,7],
        [4,0,1,2,3,9,5,6,7,8],
        [5,9,8,7,6,0,4,3,2,1],
        [6,5,9,8,7,1,0,4,3,2],
        [7,6,5,9,8,2,1,0,4,3],
        [8,7,6,5,9,3,2,1,0,4],
        [9,8,7,6,5,4,3,2,1,0]
    ]
    perm = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,5,7,6,2,8,3,0,9,4],
        [5,8,0,3,7,9,6,1,4,2],
        [8,9,1,6,0,4,3,5,2,7],
        [9,4,5,3,1,2,6,8,7,0],
        [4,2,8,6,5,7,3,9,0,1],
        [2,7,9,3,8,0,6,4,1,5],
        [7,0,4,6,9,1,3,2,5,8]
    ]
    inv = [0,4,3,2,1,5,6,7,8,9]
    c = 0
    for i, ch in enumerate(reversed(number)):
        c = mul[c][perm[i % 8][int(ch)]]
    return inv[c] == 0

# ---------- Card Luhn ----------
def _luhn_valid(num: str) -> bool:
    s, alt = 0, False
    for ch in reversed(num):
        if not ch.isdigit():
            return False
        n = ord(ch) - 48
        if alt:
            n *= 2
            if n > 9:
                n -= 9
        s += n
        alt = not alt
    return (s % 10) == 0

# ---------- KEYWORDS ----------
KEYWORD_PHRASES = {
    "ADDRESS_KEYWORD": [
        "address","full address","complete address",
        "residential address","permanent address","addr"
    ],
    "NAME_KEYWORD": [
        "name","nam","fathername","mothername","custname"
    ],
    "DOB_KEYWORD": [
        "date of birth","dob","birthdate","born on","custDob"
    ],
    "ACCOUNT_NUMBER_KEYWORD": [
        "account number","acc number","bank account",
        "account no","a/c no","accountnumber"
    ],
    "CUSTOMER_ID_KEYWORD": [
        "customer id","cust id","customer number","cust",
        "customerid","custid"
    ],
    "SENSITIVE_HINTS_KEYWORD": [
        "national id","identity card","proof of identity","document number"
    ],
    "INSURANCE_POLICY_KEYWORD": [
        "insurance number","policy number","insurance id","ins id"
    ],
}

def _build_keyword_regex_pair(phrase: str):
    # allow spaces/hyphens/underscores inside; strict alnum boundaries outside
    pat_core = re.sub(r"\s+", r"[\\s_-]+", re.escape(phrase))
    presence = re.compile(rf"(?<![A-Za-z0-9]){pat_core}(?![A-Za-z0-9])", re.IGNORECASE)
    # Optional quote between keyword and separator; capture value up to comma/semicolon/EOL
    kv = re.compile(rf"(?<![A-Za-z0-9]){pat_core}(?![A-Za-z0-9])\s*['\"]?\s*[:=\-]\s*([^;,]*)", re.IGNORECASE)
    return presence, kv

KEYWORD_REGEXES = {k: [_build_keyword_regex_pair(p) for p in v] for k, v in KEYWORD_PHRASES.items()}

# Clean folder names (ALWAYS use these for writing)
KEYWORD_FOLDER = {
    "ADDRESS_KEYWORD": "address",
    "NAME_KEYWORD": "name",
    "DOB_KEYWORD": "dob",
    "ACCOUNT_NUMBER_KEYWORD": "account_number",
    "CUSTOMER_ID_KEYWORD": "customer_id",
    "SENSITIVE_HINTS_KEYWORD": "sensitive_hints",
    "INSURANCE_POLICY_KEYWORD": "insurance_policy",
}
REGEX_FOLDER = {
    "MOBILE_REGEX": "mobile",
    "AADHAAR_REGEX": "aadhaar",
    "PAN_REGEX": "pan",
    "GSTIN_REGEX": "gstin",
    "DL_REGEX": "dl",
    "VOTERID_REGEX": "voterid",
    "EMAIL_REGEX": "email",
    "UPI_REGEX": "upi",
    "IP_REGEX": "ip",
    "MAC_REGEX": "mac",
    "CARD_REGEX": "card",
    "COORD_REGEX": "coord",
}
KEYWORD_FOLDERS_SET = set(KEYWORD_FOLDER.values())
REGEX_FOLDERS_SET = set(REGEX_FOLDER.values())

REVERSE_REGEX_FOLDER = {v: k for k, v in REGEX_FOLDER.items()}

# Which regex engines are active (pattern keys like "UPI_REGEX")
if ONLY_REGEX_TYPES:
    ALLOWED_RTYPE_SET = {REVERSE_REGEX_FOLDER[t] for t in ONLY_REGEX_TYPES if t in REVERSE_REGEX_FOLDER}
else:
    ALLOWED_RTYPE_SET = set(PII_PATTERNS.keys())

# Keywords toggle & allowlist (by folder names like "name","address")
ENABLE_KEYWORDS     = bool(ONLY_KEYWORD_TYPES)
ALLOWED_KW_FOLDERS  = set(ONLY_KEYWORD_TYPES) if ENABLE_KEYWORDS else set()

# ---------- Helpers ----------
def _strip_wrappers(val: str) -> str:
    v = val.strip()
    wrappers = [('"','"'), ("'", "'"), ('[',']'), ('(',')'), ('{','}'), ('<','>')]
    changed = True
    while changed and len(v) >= 2:
        changed = False
        for L, R in wrappers:
            if v.startswith(L) and v.endswith(R):
                v = v[1:-1].strip()
                changed = True
    return v

# Mobile token AT START (accept optional leading '+', optional spaces/hyphens after 91)
_MOBILE_AT_START = re.compile(r'^(?:\+?91[\s-]*)?[6-9]\d{9}(?!\d)')

def is_mobile_value_start(val: str) -> bool:
    return bool(_MOBILE_AT_START.match(_strip_wrappers(val)))

def _first5_alnum(value: str) -> str:
    return re.sub(r"[^A-Za-z0-9]", "", _strip_wrappers(value))[:5]

# ---------- Prefilters (speedups) ----------
KW_PREFILTER_TOKENS = (
    "name", "nam", "fathername", "mothername", "custname",
    "address", "addr", "full address", "complete address",
    "residential address", "permanent address",
    "date of birth", "dob", "birthdate", "born on", "custdob",
    "account", "acc number", "account number", "account no", "a/c no", "bank account", "accountnumber",
    "customer", "cust id", "customer id", "customer number", "customerid", "custid",
    "national id", "identity card", "proof of identity", "document number",
    "insurance number", "policy number", "insurance id", "ins id",
)
_HAS_DIGIT = re.compile(r"\d")

# ---------- Matching ----------
def should_skip_keyword(ktype: str, value: str) -> bool:
    v_raw = value.strip().strip(",")
    v = _strip_wrappers(v_raw)
    v_lower = v.lower()

    # Global skip rules
    if v == "" or v_lower == "null":
        return True
    if re.fullmatch(r"\*+", v):            # masked "****"
        return True
    if v_lower in ("y", "n"):               # flag-only
        return True

    # Address special case
    if ktype == "ADDRESS_KEYWORD" and v_lower == "ip-address":
        return True

    # Account number / Customer ID (all variants)
    if ktype in ("ACCOUNT_NUMBER_KEYWORD", "CUSTOMER_ID_KEYWORD"):
        if "mobile" in v_lower:
            return True
        if is_mobile_value_start(v):
            return True
        al5 = _first5_alnum(v)
        if al5 and not al5.isdigit():
            return True

    return False

def keyword_matches_for_line(line: str):
    valid_types = set()
    skipped = defaultdict(int)

    # Cheap prefilter: only run heavy keyword regexes if likely relevant
    ll = line.lower()
    has_kw_token = any(tok in ll for tok in KW_PREFILTER_TOKENS)
    has_kv_sep = (':' in line) or ('=' in line) or ('-' in line)
    if not (has_kw_token or has_kv_sep):
        return valid_types, skipped  # nothing to find

    for ktype, pairs in KEYWORD_REGEXES.items():
        folder = KEYWORD_FOLDER[ktype]

        any_value_seen = False
        any_value_kept = False
        bare_mention = False
        local_skips = 0

        for presence_rx, kv_rx in pairs:
            m = kv_rx.search(line)
            if m:
                any_value_seen = True
                value = m.group(1)
                if should_skip_keyword(ktype, value):
                    local_skips += 1
                else:
                    any_value_kept = True
            else:
                if presence_rx.search(line):
                    bare_mention = True

        if any_value_kept or (not any_value_seen and bare_mention):
            valid_types.add(folder)
        if local_skips:
            skipped[folder] += local_skips

    return valid_types, skipped

def regex_matches_for_line(line: str):
    types = set()
    has_digit = bool(_HAS_DIGIT.search(line))
    allowed = ALLOWED_RTYPE_SET

    # Email / UPI need '@'
    if '@' in line:
        if "EMAIL_REGEX" in allowed and PII_PATTERNS["EMAIL_REGEX"].search(line):
            types.add(REGEX_FOLDER["EMAIL_REGEX"])
        if "UPI_REGEX" in allowed and PII_PATTERNS["UPI_REGEX"].search(line):
            types.add(REGEX_FOLDER["UPI_REGEX"])

    if has_digit:
        # Mobile
        if "MOBILE_REGEX" in allowed and PII_PATTERNS["MOBILE_REGEX"].search(line):
            types.add(REGEX_FOLDER["MOBILE_REGEX"])

        # Numeric-heavy IDs
        for rtype in ("AADHAAR_REGEX", "PAN_REGEX", "GSTIN_REGEX", "DL_REGEX", "VOTERID_REGEX", "CARD_REGEX"):
            if rtype not in allowed:
                continue
            rx = PII_PATTERNS[rtype]
            for m in rx.finditer(line):
                val = m.group(1) if (rtype == "AADHAAR_REGEX" and m.lastindex) else m.group(0)
                if rtype == "AADHAAR_REGEX" and not is_valid_aadhaar(val):
                    continue
                if rtype == "CARD_REGEX" and not _luhn_valid(val):
                    continue
                types.add(REGEX_FOLDER[rtype])
                break

        # IPv4
        if '.' in line and "IP_REGEX" in allowed and PII_PATTERNS["IP_REGEX"].search(line):
            types.add(REGEX_FOLDER["IP_REGEX"])

        # Coords
        if ',' in line and '.' in line and "COORD_REGEX" in allowed and PII_PATTERNS["COORD_REGEX"].search(line):
            types.add(REGEX_FOLDER["COORD_REGEX"])

    # MAC
    if (':' in line or '-' in line) and "MAC_REGEX" in allowed and PII_PATTERNS["MAC_REGEX"].search(line):
        types.add(REGEX_FOLDER["MAC_REGEX"])

    return types

# ---------- Writer (split by 10k), runs in parent thread ----------
class WriterManager:
    def __init__(self, base_out: Path, lines_per_file: int):
        self.base_out = base_out
        self.lines_per_file = lines_per_file
        self.handles = {}  # (kind, folder) -> (fh, current_count)
        self.file_index = defaultdict(int)  # (kind, folder) -> idx
        self.total_lines_by_type = defaultdict(int)   # folder -> lines written
        self.files_created_by_type = defaultdict(int) # folder -> num files created

    def _open_handle(self, kind: str, folder: str):
        root = "keyword_matches" if kind == "keyword" else "regex_matches"
        out_dir = self.base_out / root / folder
        out_dir.mkdir(parents=True, exist_ok=True)
        idx = self.file_index[(kind, folder)]
        if idx == 0:
            idx = 1
        fname = f"{folder}_{idx:04d}.txt"
        fpath = out_dir / fname
        fh = open(fpath, "a", encoding="utf-8", buffering=1 << 20)  # 1 MiB buffer
        self.handles[(kind, folder)] = (fh, 0)
        if self.files_created_by_type[folder] < idx:
            self.files_created_by_type[folder] = idx

    def write(self, kind: str, folder: str, line: str):
        key = (kind, folder)
        if key not in self.handles:
            self.file_index[key] = max(1, self.file_index[key])
            self._open_handle(kind, folder)
        fh, cur = self.handles[key]
        if cur >= self.lines_per_file:
            fh.close()
            self.file_index[key] += 1
            self._open_handle(kind, folder)
            fh, cur = self.handles[key]
        fh.write(line + "\n")
        self.handles[key] = (fh, cur + 1)
        self.total_lines_by_type[folder] += 1

    def close(self):
        for fh, _ in list(self.handles.values()):
            try:
                fh.close()
            except:
                pass

def writer_consumer(queue: "mp.SimpleQueue", writer: WriterManager, expected_done: int):
    done = 0
    while True:
        msg = queue.get()
        if msg[0] == 'batch':
            for kind, folder, line in msg[1]:
                writer.write(kind, folder, line)
        elif msg[0] == 'done':
            done += 1
            if done >= expected_done:
                break

# ---------- Worker (process) ----------
_OUT_QUEUE = None  # set by initializer

def _init_worker(q: "mp.SimpleQueue"):
    global _OUT_QUEUE
    _OUT_QUEUE = q

def keyword_pipeline(line: str):
    if not ENABLE_KEYWORDS:
        return set(), {}
    valid_types, skipped = keyword_matches_for_line(line)
    valid_types = {t for t in valid_types if t in ALLOWED_KW_FOLDERS}
    return valid_types, skipped

def process_file_worker(file_path: str, batch_size: int):
    local = {
        "lines_total": 0,
        "lines_dropped": 0,
        "skipped_keywords": defaultdict(int),
        "failed": None,
        "errors": [],
    }
    try:
        batch = []
        with open(file_path, "r", encoding="utf-8", errors="ignore", buffering=1 << 20) as f:
            for raw in f:
                line = raw.rstrip("\n")
                if not line.strip():
                    continue
                local["lines_total"] += 1

                kw_types, kw_skips = keyword_pipeline(line)
                rx_types = regex_matches_for_line(line)

                for k, v in kw_skips.items():
                    local["skipped_keywords"][k] += v

                wrote_any = False
                for folder in kw_types:
                    batch.append(("keyword", folder, line))
                    wrote_any = True
                for folder in rx_types:
                    batch.append(("regex", folder, line))
                    wrote_any = True

                if not wrote_any:
                    local["lines_dropped"] += 1

                if len(batch) >= batch_size:
                    _OUT_QUEUE.put(('batch', batch))
                    batch = []

        if batch:
            _OUT_QUEUE.put(('batch', batch))

    except Exception as e:
        local["failed"] = file_path
        local["errors"].append(f"{file_path}: {e}\n{traceback.format_exc()}")

    local["skipped_keywords"] = dict(local["skipped_keywords"])
    _OUT_QUEUE.put(('done', None))
    return local

# ---------- Summary ----------
def write_summary(summary):
    total_lines_written = sum(summary["per_type_counts"].values())
    total_files_written = sum(summary["output_files_per_type"].values())
    total_files_written_keyword = sum(
        count for folder, count in summary["output_files_per_type"].items()
        if folder in summary["keyword_folders_set"]
    )
    total_files_written_regex = sum(
        count for folder, count in summary["output_files_per_type"].items()
        if folder in summary["regex_folders_set"]
    )

    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write("Summary Report\n")
        f.write(f"Input Folder: {INPUT_FOLDER}\n")
        f.write(f"Output Folder: {OUTPUT_FOLDER}\n\n")

        f.write(f"Total Lines Written: {total_lines_written}\n")
        f.write(f"Total Input Files Scanned: {summary['files_scanned']}\n")
        f.write(f"Total Files Written: {total_files_written}\n")
        f.write(f"Total Files Written (Regex Types): {total_files_written_regex}\n")
        f.write(f"Total Files Written (Keyword Types): {total_files_written_keyword}\n\n")

        f.write(f"Total Lines Scanned: {summary['lines_scanned']}\n")
        f.write(f"Total Lines Dropped (no matches): {summary['lines_dropped']}\n\n")

        f.write("Per-Type Counts (lines written):\n")
        for t in sorted(summary["per_type_counts"].keys()):
            f.write(f"  {t}: {summary['per_type_counts'][t]}\n")

        f.write("\nTotal Output Files Created per PII Type:\n")
        for t in sorted(summary["output_files_per_type"].keys()):
            f.write(f"  {t}: {summary['output_files_per_type"][t]}\n")

        f.write("\nSkipped Keyword Matches (per type):\n")
        for t in sorted(summary["skipped_keywords"].keys()):
            f.write(f"  {t}: {summary['skipped_keywords"][t]}\n")

        if summary["failed_files"]:
            f.write("\nFailed Files:\n")
            for p in summary["failed_files"]:
                f.write(f"  {p}\n")

        if summary["errors"]:
            f.write("\nErrors:\n")
            for e in summary["errors"]:
                f.write(f"  {e}\n")

# ---------- Main ----------
def main():
    in_root = Path(INPUT_FOLDER)
    out_root = Path(OUTPUT_FOLDER)
    out_root.mkdir(parents=True, exist_ok=True)

    files = [str(p) for p in sorted(in_root.rglob("*.txt"))]
    summary = {
        "files_scanned": len(files),
        "lines_scanned": 0,
        "lines_dropped": 0,
        "per_type_counts": defaultdict(int),
        "output_files_per_type": defaultdict(int),
        "skipped_keywords": defaultdict(int),
        "failed_files": [],
        "errors": [],
        "keyword_folders_set": KEYWORD_FOLDERS_SET,
        "regex_folders_set": REGEX_FOLDERS_SET,
    }

    writer = WriterManager(out_root, LINES_PER_FILE)

    # Fast IPC queue for batches
    q = mp.SimpleQueue()

    # Start writer consumer in parent
    expected_done = len(files)  # ← number of tasks, not number of processes
    writer_thread = Thread(target=writer_consumer, args=(q, writer, expected_done), daemon=True)
    writer_thread.start()

    # Launch workers
    with ProcessPoolExecutor(max_workers=MAX_WORKERS, initializer=_init_worker, initargs=(q,)) as ex:
        futures = {ex.submit(process_file_worker, p, BATCH_SIZE): p for p in files}
        for fut in tqdm(as_completed(futures), total=len(futures), desc="Files"):
            result = fut.result()
            summary["lines_scanned"] += result["lines_total"]
            summary["lines_dropped"] += result["lines_dropped"]
            for k, v in result["skipped_keywords"].items():
                summary["skipped_keywords"][k] += v
            if result["failed"]:
                summary["failed_files"].append(result["failed"])
            if result["errors"]:
                summary["errors"].extend(result["errors"])

    # Wait for writer to finish (after all 'done' tokens)
    writer_thread.join()
    writer.close()

    # Populate per-type counts and created files from the writer
    for folder, count in writer.total_lines_by_type.items():
        summary["per_type_counts"][folder] = count
    for folder, files_created in writer.files_created_by_type.items():
        summary["output_files_per_type"][folder] = files_created

    write_summary(summary)

if __name__ == "__main__":
    main()
