#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Global deduplication script for keyword extraction outputs.

- Input format:
  log line ; path ; input file path ; keyword ; keyword type ; matched value

- Deduplication key:
  (path [2nd field], matched value [last field])
  → Keep first occurrence globally across ALL files, drop later ones.

- Features:
  * ProcessPoolExecutor for parallel parsing
  * Deduplication in main process (guaranteed global uniqueness)
  * Output chunked into 10,000 lines each (chunk0001.txt, …)
  * Summary report with:
      - Total files, lines scanned, lines kept, duplicates removed
      - Per-file stats
      - Per-key duplicate counts (path + matched value)
      - Example duplicates
      - Errors
  * Error log + resume log
"""

import argparse
import concurrent.futures
import sqlite3
from collections import defaultdict, Counter
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Any

from tqdm import tqdm

# =========================
# Config
# =========================
DEFAULT_MAX_WORKERS = 6
ALLOWED_EXTS = (".txt",)
CHUNK_SIZE = 10_000
SUMMARY_FILENAME = "summary_dedup.txt"
ERROR_LOG = "error.log"
RESUME_LOG = "resume_files.log"

# =========================
# SQLite helpers
# =========================
def ensure_db(path: Path) -> sqlite3.Connection:
    path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(str(path))
    conn.execute("""
        CREATE TABLE IF NOT EXISTS processed_files (
            file_path TEXT PRIMARY KEY,
            mtime REAL,
            size INTEGER,
            processed_at TEXT
        )
    """)
    conn.commit()
    return conn

def file_fingerprint(p: Path):
    st = p.stat()
    return (st.st_mtime, st.st_size)

def is_already_processed(conn: sqlite3.Connection, file_path: str, mtime: float, size: int) -> bool:
    cur = conn.execute("SELECT mtime, size FROM processed_files WHERE file_path = ?", (file_path,))
    row = cur.fetchone()
    if not row:
        return False
    return (abs(row[0] - mtime) < 1e-6) and (row[1] == size)

def mark_processed(conn: sqlite3.Connection, file_path: str, mtime: float, size: int):
    conn.execute("""
        INSERT INTO processed_files(file_path, mtime, size, processed_at)
        VALUES (?, ?, ?, ?)
        ON CONFLICT(file_path) DO UPDATE SET mtime=excluded.mtime, size=excluded.size, processed_at=excluded.processed_at
    """, (file_path, mtime, size, datetime.now().isoformat(timespec='seconds')))
    conn.commit()

# =========================
# Chunk writer
# =========================
@dataclass
class ChunkState:
    out_dir: Path
    chunk_idx: int = 1
    lines_in_chunk: int = 0
    handle: Any = None

    def ensure_handle(self):
        if self.handle is None:
            self.out_dir.mkdir(parents=True, exist_ok=True)
            fn = self.out_dir / f"chunk{self.chunk_idx:04d}.txt"
            self.handle = fn.open("w", encoding="utf-8", errors="ignore")

    def write_line(self, line: str):
        self.ensure_handle()
        self.handle.write(line + "\n")
        self.lines_in_chunk += 1
        if self.lines_in_chunk >= CHUNK_SIZE:
            self.handle.close()
            self.handle = None
            self.chunk_idx += 1
            self.lines_in_chunk = 0

    def close(self):
        if self.handle:
            self.handle.close()
            self.handle = None

# =========================
# Worker
# =========================
@dataclass
class FileStats:
    lines_scanned: int = 0
    parsed_lines: int = 0
    errors: int = 0

def process_file(file_path: str) -> Dict:
    stats = FileStats()
    parsed: List[str] = []
    examples_bad: List[str] = []
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            for raw in f:
                line = raw.rstrip("\n")
                stats.lines_scanned += 1
                parts = line.rsplit(";", 5)
                if len(parts) != 6:
                    stats.errors += 1
                    if len(examples_bad) < 5:
                        examples_bad.append(f"Malformed: {line[:200]}")
                    continue
                parsed.append(line)
                stats.parsed_lines += 1
    except Exception as e:
        examples_bad.append(f"{file_path} :: {type(e).__name__}: {e}")
    return {
        "file": file_path,
        "stats": stats.__dict__,
        "lines": parsed,
        "examples_bad": examples_bad,
    }

# =========================
# Summary
# =========================
def write_summary(path: Path,
                  input_root: Path,
                  output_root: Path,
                  total_files: int,
                  total_lines: int,
                  total_kept: int,
                  total_dropped: int,
                  per_file_stats: Dict[str, FileStats],
                  per_key_dupes: Counter,
                  examples_dupes: List[str],
                  errors: List[str]):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(path, "w", encoding="utf-8") as f:
        f.write("Summary Report - Global Deduplication\n")
        f.write(f"Run Completed: {now}\n\n")
        f.write(f"Input Folder: {input_root}\n")
        f.write(f"Output Folder: {output_root}\n\n")

        f.write("-------------------------\nOverall Stats\n-------------------------\n")
        f.write(f"Total Files Scanned   : {total_files}\n")
        f.write(f"Total Lines Scanned   : {total_lines}\n")
        f.write(f"Total Unique Lines    : {total_kept}\n")
        f.write(f"Total Duplicates      : {total_dropped}\n\n")

        f.write("-------------------------\nPer-File Stats\n-------------------------\n")
        for fname, s in per_file_stats.items():
            f.write(f"{fname}\n")
            f.write(f"  Lines Scanned: {s.lines_scanned}\n")
            f.write(f"  Parsed: {s.parsed_lines}\n")
            f.write(f"  Malformed: {s.errors}\n\n")

        f.write("-------------------------\nTop Duplicate Keys\n-------------------------\n")
        for (path_val, match_val), count in per_key_dupes.most_common(20):
            f.write(f"{path_val} + {match_val} → {count} duplicates removed\n")
        f.write("\n")

        f.write("-------------------------\nExamples of Duplicates\n-------------------------\n")
        for ex in examples_dupes:
            f.write(ex + "\n")
        f.write("\n")

        f.write("-------------------------\nErrors\n-------------------------\n")
        if not errors:
            f.write("(No errors)\n")
        else:
            for e in errors:
                f.write(e + "\n")

# =========================
# Main
# =========================
def main():
    parser = argparse.ArgumentParser(description="Global deduplication of keyword extraction outputs")
    parser.add_argument("--input", required=True, help="Input folder with .txt files")
    parser.add_argument("--output", required=True, help="Output folder for deduplicated chunks")
    parser.add_argument("--workers", type=int, default=DEFAULT_MAX_WORKERS, help="Worker processes")
    args = parser.parse_args()

    input_root = Path(args.input).resolve()
    output_root = Path(args.output).resolve()
    output_root.mkdir(parents=True, exist_ok=True)

    db_path = output_root / ".dedup_index.sqlite"
    error_path = output_root / ERROR_LOG
    summary_path = output_root / SUMMARY_FILENAME
    resume_log_path = output_root / RESUME_LOG

    conn = ensure_db(db_path)
    files = sorted([p for p in input_root.glob("*.txt")])

    total_files = 0
    total_lines = 0
    total_kept = 0
    total_dropped = 0
    per_file_stats: Dict[str, FileStats] = {}
    per_key_dupes: Counter = Counter()
    examples_dupes: List[str] = []
    errors: List[str] = []

    # Step 1: parse all files in parallel
    all_lines: List[Tuple[str, str, str, str]] = []  # (raw line, path, matched value, file)
    with concurrent.futures.ProcessPoolExecutor(max_workers=args.workers) as ex:
        futs = {ex.submit(process_file, str(p)): p for p in files}
        for fut in tqdm(concurrent.futures.as_completed(futs), total=len(futs), desc="Parse"):
            p = futs[fut]
            try:
                res = fut.result()
            except Exception as e:
                errors.append(f"[future] {p}: {e}")
                continue
            stats = FileStats(**res["stats"])
            per_file_stats[str(p)] = stats
            total_files += 1
            total_lines += stats.lines_scanned
            for line in res["lines"]:
                parts = line.rsplit(";", 5)
                _, path_val, _, _, _, matched_val = [p.strip() for p in parts]
                all_lines.append((line, path_val, matched_val, str(p)))
            for b in res["examples_bad"]:
                errors.append(b)
            try:
                mtime, size = file_fingerprint(p)
                mark_processed(conn, str(p), mtime, size)
                with open(resume_log_path, "a", encoding="utf-8") as rlog:
                    rlog.write(f"{datetime.now().isoformat(timespec='seconds')} PROCESSED {p}\n")
            except Exception as e:
                errors.append(f"[mark_processed] {p}: {e}")

    # Step 2: deduplicate globally
    seen = set()
    unique_lines = []
    for raw, path_val, matched_val, fname in all_lines:
        key = (path_val, matched_val)
        if key in seen:
            total_dropped += 1
            per_key_dupes[key] += 1
            if len(examples_dupes) < 10:
                examples_dupes.append(f"Duplicate in {fname}: {path_val} + {matched_val}")
            continue
        seen.add(key)
        unique_lines.append(raw)
        total_kept += 1

    # Step 3: write deduplicated output
    chunk_state = ChunkState(out_dir=output_root)
    for line in unique_lines:
        chunk_state.write_line(line)
    chunk_state.close()

    # Step 4: summary
    write_summary(summary_path, input_root, output_root, total_files, total_lines,
                  total_kept, total_dropped, per_file_stats, per_key_dupes,
                  examples_dupes, errors)

    print(f"Done. Dedup summary written to {summary_path}")

if __name__ == "__main__":
    main()
