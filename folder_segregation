#!/usr/bin/env python3
import os
import re
import sys
import traceback
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from collections import defaultdict
from tqdm import tqdm

# ============= CONFIG ============= #
INPUT_FOLDER     = "input_logs"          # put your input folder name here (in current dir)
OUTPUT_FOLDER    = "output_pii"          # outputs will be created under this
SUMMARY_FILE     = "summary_report.txt"  # summary file in current dir
MAX_WORKERS      = 6                     # parallel workers
LINES_PER_FILE   = 10000                 # split size per PII type file
# ================================== #

# ---------- REGEX PATTERNS ----------
PII_PATTERNS = {
    "MOBILE_REGEX": re.compile(r'(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])'),
    "AADHAAR_REGEX": re.compile(r'(?<![A-Za-z0-9])(\d{12})(?![A-Za-z0-9])'),
    "PAN_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{5}\d{4}[A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "GSTIN_REGEX": re.compile(r'(?<![A-Za-z0-9])\d{2}[A-Z]{5}\d{4}[A-Z][1-9A-Z]Z[0-9A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "DL_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{2}\d{2}\d{11}(?![A-Za-z0-9])', re.IGNORECASE),
    "VOTERID_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{3}\d{7}(?![A-Za-z0-9])', re.IGNORECASE),
    "EMAIL_REGEX": re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', re.IGNORECASE),
    "UPI_REGEX": re.compile(r'[A-Za-z0-9._-]{2,256}@[A-Za-z]{2,64}(?=$|[\s<>\[\]\{\}\(\),;_\-])'),
    "IP_REGEX": re.compile(
        r'(?<!\d)'
        r'(?:(?:25[0-5]|2[0-4]\d|1?\d?\d)\.){3}'
        r'(?:25[0-5]|2[0-4]\d|1?\d?\d)'
        r'(?!\d)'
    ),
    "MAC_REGEX": re.compile(r'(?<![A-Fa-f0-9])(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}(?![A-Fa-f0-9])'),
    "CARD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])('
        r'4\d{12}(?:\d{3})?'                                 # Visa
        r'|5[1-5]\d{14}'                                     # Mastercard (old range)
        r'|2(?:2[2-9]\d{12}|[3-6]\d{13}|7(?:[01]\d{12}|20\d{12}))'
        r'|3[47]\d{13}'                                      # Amex
        r'|60\d{14}|65\d{14}|81\d{14}|508\d\d{12}'           # Other BINs
        r')(?![A-Za-z0-9])'
    ),
    "COORD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])'
        r'([+-]?(?:90\.(?:0+)?|[0-8]?\d\.\d+))'
        r'\s*,\s*'
        r'([+-]?(?:180\.(?:0+)?|1[0-7]\d\.\d+|[0-9]?\d\.\d+))'
        r'(?![A-Za-z0-9])'
    ),
}

# ---------- Aadhaar Verhoeff ----------
@lru_cache(maxsize=10000)
def is_valid_aadhaar(number: str) -> bool:
    if len(number) != 12 or not number.isdigit():
        return False
    mul = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,2,3,4,0,6,7,8,9,5],
        [2,3,4,0,1,7,8,9,5,6],
        [3,4,0,1,2,8,9,5,6,7],
        [4,0,1,2,3,9,5,6,7,8],
        [5,9,8,7,6,0,4,3,2,1],
        [6,5,9,8,7,1,0,4,3,2],
        [7,6,5,9,8,2,1,0,4,3],
        [8,7,6,5,9,3,2,1,0,4],
        [9,8,7,6,5,4,3,2,1,0]
    ]
    perm = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,5,7,6,2,8,3,0,9,4],
        [5,8,0,3,7,9,6,1,4,2],
        [8,9,1,6,0,4,3,5,2,7],
        [9,4,5,3,1,2,6,8,7,0],
        [4,2,8,6,5,7,3,9,0,1],
        [2,7,9,3,8,0,6,4,1,5],
        [7,0,4,6,9,1,3,2,5,8]
    ]
    inv = [0,4,3,2,1,5,6,7,8,9]
    c = 0
    for i, ch in enumerate(reversed(number)):
        c = mul[c][perm[i % 8][int(ch)]]
    return inv[c] == 0

# ---------- Card Luhn ----------
def _luhn_valid(num: str) -> bool:
    s, alt = 0, False
    for ch in reversed(num):
        if not ch.isdigit():
            return False
        n = ord(ch) - 48
        if alt:
            n *= 2
            if n > 9:
                n -= 9
        s += n
        alt = not alt
    return (s % 10) == 0

# ---------- KEYWORDS ----------
KEYWORD_PHRASES = {
    "ADDRESS_KEYWORD": [
        "address","full address","complete address",
        "residential address","permanent address","addr"
    ],
    "NAME_KEYWORD": [
        "name","nam","fathername","mothername","custname"
    ],
    "DOB_KEYWORD": [
        "date of birth","dob","birthdate","born on","custDob"
    ],
    "ACCOUNT_NUMBER_KEYWORD": [
        "account number","acc number","bank account",
        "account no","a/c no","accountnumber"
    ],
    "CUSTOMER_ID_KEYWORD": [
        "customer id","cust id","customer number","cust"
    ],
    "SENSITIVE_HINTS_KEYWORD": [
        "national id","identity card","proof of identity","document number"
    ],
    "INSURANCE_POLICY_KEYWORD": [
        "insurance number","policy number","insurance id","ins id"
    ],
}

def build_keyword_regex(phrase: str) -> re.Pattern:
    # allow spaces, hyphens, underscores inside phrase; strict alpha-num boundaries outside
    pat = re.sub(r"\s+", r"[\\s_-]+", re.escape(phrase))
    return re.compile(rf"(?<![A-Za-z0-9]){pat}(?![A-Za-z0-9])", re.IGNORECASE)

KEYWORD_REGEXES = {k: [build_keyword_regex(p) for p in v] for k, v in KEYWORD_PHRASES.items()}

# Clean folder names
KEYWORD_FOLDER = {
    "ADDRESS_KEYWORD": "address",
    "NAME_KEYWORD": "name",
    "DOB_KEYWORD": "dob",
    "ACCOUNT_NUMBER_KEYWORD": "account_number",
    "CUSTOMER_ID_KEYWORD": "customer_id",
    "SENSITIVE_HINTS_KEYWORD": "sensitive_hints",
    "INSURANCE_POLICY_KEYWORD": "insurance_policy",
}
REGEX_FOLDER = {
    "MOBILE_REGEX": "mobile",
    "AADHAAR_REGEX": "aadhaar",
    "PAN_REGEX": "pan",
    "GSTIN_REGEX": "gstin",
    "DL_REGEX": "dl",
    "VOTERID_REGEX": "voterid",
    "EMAIL_REGEX": "email",
    "UPI_REGEX": "upi",
    "IP_REGEX": "ip",
    "MAC_REGEX": "mac",
    "CARD_REGEX": "card",
    "COORD_REGEX": "coord",
}
KEYWORD_FOLDERS_SET = set(KEYWORD_FOLDER.values())
REGEX_FOLDERS_SET = set(REGEX_FOLDER.values())

# ---------- Helpers ----------
def _extract_value_after(phrase_regex: re.Pattern, line: str) -> str | None:
    m = re.search(rf"{phrase_regex.pattern}\s*[:=\-]\s*([^;,]*)", line, re.IGNORECASE)
    return m.group(1) if m else None

def _first5_alnum(value: str) -> str:
    return re.sub(r"[^A-Za-z0-9]", "", value)[:5]

def is_mobile_value(val: str) -> bool:
    return bool(PII_PATTERNS["MOBILE_REGEX"].fullmatch(val.strip()))

def should_skip_keyword(ktype: str, value: str) -> bool:
    v = value.strip().strip(",").strip().strip('"').strip("'")
    v_lower = v.lower()
    if v == "" or v_lower == "null":
        return True
    if re.fullmatch(r"\*+", v):
        return True
    if v_lower in ("y", "n"):
        return True
    if ktype == "ADDRESS_KEYWORD" and v_lower == "ip-address":
        return True
    if ktype in ("ACCOUNT_NUMBER_KEYWORD", "CUSTOMER_ID_KEYWORD"):
        if is_mobile_value(v):
            return True
        al5 = _first5_alnum(v)
        if al5 and not al5.isdigit():
            return True
    return False

def keyword_matches_for_line(line: str):
    valid_types = set()
    skipped = defaultdict(int)
    for ktype, regexes in KEYWORD_REGEXES.items():
        folder = KEYWORD_FOLDER[ktype]
        found_valid = False
        local_skips = 0
        for rx in regexes:
            if not rx.search(line):
                continue
            value = _extract_value_after(rx, line)
            if value is not None:
                if should_skip_keyword(ktype, value):
                    local_skips += 1
                    continue
                else:
                    found_valid = True
            else:
                found_valid = True
        if found_valid:
            valid_types.add(folder)
        if local_skips:
            skipped[folder] += local_skips
    return valid_types, skipped

def regex_matches_for_line(line: str):
    types = set()
    for rtype, rx in PII_PATTERNS.items():
        for m in rx.finditer(line):
            val = m.group(1) if (rtype == "AADHAAR_REGEX" and m.lastindex) else m.group(0)
            if rtype == "AADHAAR_REGEX" and not is_valid_aadhaar(val):
                continue
            if rtype == "CARD_REGEX" and not _luhn_valid(val):
                continue
            types.add(REGEX_FOLDER[rtype])
            break
    return types

# ---------- Writer (split by 10k) ----------
class WriterManager:
    def __init__(self, base_out: Path, lines_per_file: int):
        self.base_out = base_out
        self.lines_per_file = lines_per_file
        self.handles = {}  # (kind, folder) -> (fh, current_count)
        self.file_index = defaultdict(int)  # (kind, folder) -> idx
        self.total_lines_by_type = defaultdict(int)   # folder -> lines written
        self.files_created_by_type = defaultdict(int) # folder -> num files created

    def _open_handle(self, kind: str, folder: str):
        root = "keyword_matches" if kind == "keyword" else "regex_matches"
        out_dir = self.base_out / root / folder
        out_dir.mkdir(parents=True, exist_ok=True)
        idx = self.file_index[(kind, folder)]
        if idx == 0:
            idx = 1
        fname = f"{folder}_{idx:04d}.txt"
        fpath = out_dir / fname
        fh = open(fpath, "a", encoding="utf-8")
        self.handles[(kind, folder)] = (fh, 0)
        if self.files_created_by_type[folder] < idx:
            self.files_created_by_type[folder] = idx

    def write(self, kind: str, folder: str, line: str):
        key = (kind, folder)
        if key not in self.handles:
            self.file_index[key] = max(1, self.file_index[key])
            self._open_handle(kind, folder)
        fh, cur = self.handles[key]
        if cur >= self.lines_per_file:
            fh.close()
            self.file_index[key] += 1
            self._open_handle(kind, folder)
            fh, cur = self.handles[key]
        fh.write(line + "\n")
        self.handles[key] = (fh, cur + 1)
        self.total_lines_by_type[folder] += 1

    def close(self):
        for fh, _ in list(self.handles.values()):
            try:
                fh.close()
            except:
                pass

# ---------- Worker ----------
def process_file(file_path: Path):
    local = {
        "lines_total": 0,
        "lines_dropped": 0,
        "skipped_keywords": defaultdict(int),
        "results": [],  # list[(kind, folder, line)]
        "failed": None,
        "errors": [],
    }
    try:
        with file_path.open("r", encoding="utf-8", errors="ignore") as f:
            for raw in f:
                line = raw.rstrip("\n")
                if not line.strip():
                    continue
                local["lines_total"] += 1

                kw_types, kw_skips = keyword_matches_for_line(line)
                rx_types = regex_matches_for_line(line)
                for k, v in kw_skips.items():
                    local["skipped_keywords"][k] += v

                wrote_any = False
                for folder in kw_types:
                    local["results"].append(("keyword", folder, line))
                    wrote_any = True
                for folder in rx_types:
                    local["results"].append(("regex", folder, line))
                    wrote_any = True

                if not wrote_any:
                    local["lines_dropped"] += 1

    except Exception as e:
        local["failed"] = str(file_path)
        local["errors"].append(f"{file_path}: {e}\n{traceback.format_exc()}")

    return local

# ---------- Summary ----------
def write_summary(summary):
    total_lines_written = sum(summary["per_type_counts"].values())
    total_files_written = sum(summary["output_files_per_type"].values())
    total_files_written_keyword = sum(
        count for folder, count in summary["output_files_per_type"].items()
        if folder in summary["keyword_folders_set"]
    )
    total_files_written_regex = sum(
        count for folder, count in summary["output_files_per_type"].items()
        if folder in summary["regex_folders_set"]
    )

    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write("Summary Report\n")
        f.write(f"Input Folder: {INPUT_FOLDER}\n")
        f.write(f"Output Folder: {OUTPUT_FOLDER}\n\n")

        # Your new totals
        f.write(f"Total Lines Written: {total_lines_written}\n")
        f.write(f"Total Input Files Scanned: {summary['files_scanned']}\n")
        f.write(f"Total Files Written: {total_files_written}\n")
        f.write(f"Total Files Written (Regex Types): {total_files_written_regex}\n")
        f.write(f"Total Files Written (Keyword Types): {total_files_written_keyword}\n\n")

        f.write(f"Total Lines Scanned: {summary['lines_scanned']}\n")
        f.write(f"Total Lines Dropped (no matches): {summary['lines_dropped']}\n\n")

        f.write("Per-Type Counts (lines written):\n")
        for t in sorted(summary["per_type_counts"].keys()):
            f.write(f"  {t}: {summary['per_type_counts'][t]}\n")

        f.write("\nTotal Output Files Created per PII Type:\n")
        for t in sorted(summary["output_files_per_type"].keys()):
            f.write(f"  {t}: {summary['output_files_per_type'][t]}\n")

        f.write("\nSkipped Keyword Matches (per type):\n")
        for t in sorted(summary["skipped_keywords"].keys()):
            f.write(f"  {t}: {summary['skipped_keywords'][t]}\n")

        if summary["failed_files"]:
            f.write("\nFailed Files:\n")
            for p in summary["failed_files"]:
                f.write(f"  {p}\n")

        if summary["errors"]:
            f.write("\nErrors:\n")
            for e in summary["errors"]:
                f.write(f"  {e}\n")

# ---------- Main ----------
def main():
    in_root = Path(INPUT_FOLDER)
    out_root = Path(OUTPUT_FOLDER)
    out_root.mkdir(parents=True, exist_ok=True)

    files = sorted(in_root.rglob("*.txt"))
    summary = {
        "files_scanned": len(files),
        "lines_scanned": 0,
        "lines_dropped": 0,
        "per_type_counts": defaultdict(int),
        "output_files_per_type": defaultdict(int),
        "skipped_keywords": defaultdict(int),
        "failed_files": [],
        "errors": [],
        # sets for summary splits
        "keyword_folders_set": KEYWORD_FOLDERS_SET,
        "regex_folders_set": REGEX_FOLDERS_SET,
    }

    writer = WriterManager(out_root, LINES_PER_FILE)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = {ex.submit(process_file, p): p for p in files}
        for fut in tqdm(as_completed(futures), total=len(futures), desc="Files"):
            result = fut.result()
            summary["lines_scanned"] += result["lines_total"]
            summary["lines_dropped"] += result["lines_dropped"]
            for k, v in result["skipped_keywords"].items():
                summary["skipped_keywords"][k] += v

            if result["failed"]:
                summary["failed_files"].append(result["failed"])
            if result["errors"]:
                summary["errors"].extend(result["errors"])

            for kind, folder, line in result["results"]:
                writer.write(kind, folder, line)
                summary["per_type_counts"][folder] += 1

    writer.close()
    for folder, _ in writer.total_lines_by_type.items():
        summary["output_files_per_type"][folder] = writer.files_created_by_type[folder]

    write_summary(summary)

if __name__ == "__main__":
    main()
