#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Keyword-based field→value extractor with:
- ProcessPoolExecutor (parallel)
- SQLite resume/dedupe (skip unchanged files)
- Mirrored output folder structure
- Chunking: 10,000 lines per output file, named chunk0001.txt, ...
- Live summary refresher (every 30s)
- Comprehensive summary with:
  - Overall stats
  - Per-folder stats (lines scanned, extracted, skipped invalid, skipped no-keyword)
  - Per-category stats (extracted, skipped)
  - Examples (first few per category)
  - Error section (and error.log on disk)
- Strict extraction grammar (NO risky “next token” guessing):
  - Colon/Equals/Dash with whitespace tolerance
  - JSON-style quotes
  - XML tags
  - Square brackets
- Folder-scoped keyword sets (each folder checks only its category keywords)
- Category validations per your final rules:
  - AccountNumber: numeric only, not mobile-like
  - CustomerId: numeric only, not mobile-like
  - Name: human-like (>=2 words), skip null/short/alnum/ids/tech
  - DOB: many date formats accepted (YY and YYYY)
  - Address: skip URLs/IPs/PIN-only and `IP-Address` key
"""

import argparse
import concurrent.futures
import os
import queue
import re
import signal
import sqlite3
import sys
import threading
import time
from collections import defaultdict, Counter
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime
from functools import partial
from pathlib import Path
from typing import Dict, List, Tuple, Iterable, Optional, Any

from tqdm import tqdm

# =========================
# Configuration defaults
# =========================
DEFAULT_MAX_WORKERS = 6
ALLOWED_EXTS = (".txt",)
CHUNK_SIZE = 10_000
SUMMARY_FILENAME = "summary_keywords.txt"
DB_NAME = ".keyword_index.sqlite"
ERROR_LOG = "error.log"
RESUME_LOG = "resume_files.log"  # simple append log of processed files

FOLDER_TO_CATEGORY = {
    "account_number": "ACCOUNT_NUMBER_KEYWORD",
    "customer_id": "CUSTOMER_ID_KEYWORD",
    "name": "NAME_KEYWORD",
    "dob": "DOB_KEYWORD",
    "address": "ADDRESS_KEYWORD",
}

# =========================
# Keyword phrases per category (case-insensitive)
# Spaces are allowed; underscores/hyphens/camelCase will be matched with regex.
# =========================
KEYWORD_PHRASES: Dict[str, List[str]] = {
    "ACCOUNT_NUMBER_KEYWORD": [
        "account number", "acc number", "bank account", "account no", "a/c no", "accountnumber"
    ],
    "CUSTOMER_ID_KEYWORD": [
        "customer id", "cust id", "customer number", "cust", "customerid", "custid"
    ],
    "NAME_KEYWORD": [
        "name", "nameFull", "full name", "nam", "fathername", "mothername", "custname"
    ],
    "DOB_KEYWORD": [
        "dob", "custDob", "date of birth", "birthdate", "born on"
    ],
    "ADDRESS_KEYWORD": [
        "address", "full address", "complete address", "residential address", "permanent address", "addr", "addressPerPincode"
    ],
}

# =========================
# Utility regexes
# =========================

# URL & IPv4 for universal skipping
URL_RE = re.compile(r'https?://', re.IGNORECASE)
IPV4_RE = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')

# Mobile-like detector (strict, non-embedded)
MOBILE_RE = re.compile(r'(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])')

# PIN-only (6 digits)
PIN6_RE = re.compile(r'^[ \t]*\d{6}[ \t]*$')

# Name heuristics
NAME_ALLOWED_RE = re.compile(r'^[A-Za-z][A-Za-z\.\-\s]*[A-Za-z]$')  # letters, spaces, dots, hyphens
HAS_DIGIT_OR_UNDERSCORE_RE = re.compile(r'[\d_]')
SHORT_TOKEN_RE = re.compile(r'^[A-Za-z]{1,3}$')  # Jio/IBM etc.

# Build a tolerant keyword token:
# - Case-insensitive
# - Spaces in phrase → match [\s_\-]* (underscores/hyphens/whitespace)
def phrase_to_keyword_pattern(phrase: str) -> str:
    # escape alphanumerics; replace spaces with a flexible separator
    parts = [re.escape(p) for p in phrase.split()]
    if len(parts) == 1:
        # also allow camelCase variants by just matching as-is case-insensitively
        return parts[0]
    # join with flexible separators
    return r'(?:' + r'[\s_\-]*'.join(parts) + r')'

# Extraction grammars:
# NOTE: We *always* capture inside delimiters/brackets/tags (no next-token guessing).
def build_extraction_regexes(keyword_tokens: List[str]) -> List[Tuple[str, re.Pattern]]:
    """
    For a given list of keyword token patterns, build regexes for:
    - JSON quoted: "keyword" : "value"
    - XML tags: <keyword> value </keyword>
    - Square brackets: [ keyword : value ]
    - Colon/Equals/Dash: keyword :/=/ - value (capture everything until a stopper; we still validate)
    """
    # Build alternation for keywords, with word boundaries-ish around
    # We'll use (?i) at compile time, so tokens needn't include (?i) here
    kw_alt = r'(?:' + r'|'.join(keyword_tokens) + r')'

    # JSON quoted key: "keyword" : "value"
    json_pat = re.compile(
        rf'"(?:\s*{kw_alt}\s*)"\s*:\s*"([^"]*)"',
        re.IGNORECASE
    )

    # XML tags: <keyword> value </keyword>
    # We must use the *same* token in open/close tag; we can't easily backref the alternation,
    # so we match generic <(something like keyword)...> and ensure it matches via a tempered token.
    # Strategy: capture the key token text separately using a lax group and validate in code.
    xml_pat = re.compile(
        rf'<\s*({kw_alt})\s*>\s*(.*?)\s*<\s*/\s*\1\s*>',
        re.IGNORECASE | re.DOTALL
    )

    # Square brackets: [ keyword : value ]
    bracket_pat = re.compile(
        rf'\[\s*({kw_alt})\s*(?::|=|-)\s*(.*?)\s*\]',
        re.IGNORECASE
    )

    # Colon/Equals/Dash (plaintext but *still* bound by a delimiter char):
    # Capture value lazily until a stopper: comma/semicolon/closing bracket/brace/tag start or EOL.
    # We’ll still validate the captured value downstream.
    ced_pat = re.compile(
        rf'({kw_alt})\s*(?::|=|-)\s*(.*?)\s*(?=,|;|\]|\}}|<|$)',
        re.IGNORECASE
    )

    return [
        ("json", json_pat),
        ("xml", xml_pat),
        ("bracket", bracket_pat),
        ("ced", ced_pat),
    ]

# =========================
# Validation rules per category
# =========================

def is_mobile_like(val: str) -> bool:
    digits = re.sub(r'\D+', '', val or '')
    if not digits:
        return False
    if len(digits) == 10 and digits[0] in "6789":
        return True
    if len(digits) == 12 and digits.startswith('91') and digits[2] in "6789":
        return True
    # Also run strict regex to avoid embedded false positives
    return bool(MOBILE_RE.search(val))

def clean_value(val: str) -> str:
    # Strip quotes/spaces, collapse internal multiple spaces
    if val is None:
        return ""
    v = val.strip().strip('"').strip("'").strip()
    v = re.sub(r'\s+', ' ', v)
    return v

def is_valid_account_number(val: str) -> bool:
    v = clean_value(val)
    if not v or not v.isdigit():
        return False
    if is_mobile_like(v):
        return False
    return True

def is_valid_customer_id(val: str) -> bool:
    v = clean_value(val)
    if not v or not v.isdigit():
        return False
    if is_mobile_like(v):
        return False
    return True

def looks_like_human_name(val: str) -> bool:
    v = clean_value(val)
    if not v:
        return False
    if v.lower() == "null":
        return False
    if HAS_DIGIT_OR_UNDERSCORE_RE.search(v):
        return False
    if SHORT_TOKEN_RE.match(v):  # Jio, IBM etc.
        return False
    # Must be at least 2 words separated by space (after trimming)
    words = [w for w in v.split() if w]
    if len(words) < 2:
        return False
    # Alphabetic with allowed punctuation
    if not NAME_ALLOWED_RE.match(v):
        return False
    # Exclude obvious technical labels
    if "pin" in v.lower():
        return False
    return True

# Date parsing — try multiple strptime formats
from datetime import datetime

DATE_FORMATS = [
    "%Y-%m-%d", "%d-%m-%Y", "%d/%m/%Y", "%d.%m.%Y",
    "%d-%b-%Y", "%d %b %Y", "%b %d %Y", "%B %d %Y",
    "%d-%b-%y", "%d %b %y", "%d %B %Y", "%d %B %y",
    "%Y/%m/%d", "%y-%m-%d", "%d/%m/%y"
]

def is_valid_dob(val: str) -> bool:
    v = clean_value(val)
    if not v:
        return False
    # Reject obvious non-dates
    if v.isdigit() and (len(v) >= 6):
        # Pure long digits → likely not a formatted date
        return False
    # Try parsing
    for fmt in DATE_FORMATS:
        try:
            _ = datetime.strptime(v, fmt)
            return True
        except ValueError:
            continue
    # Also allow forms like "12 Jan 2003" with commas removed already
    return False

def is_valid_address(field_key: str, val: str) -> bool:
    # Keyword itself may be IP-Address
    if field_key and field_key.strip().lower().replace('_','-') == "ip-address":
        return False
    v = clean_value(val)
    if not v:
        return False
    if URL_RE.search(v):
        return False
    if IPV4_RE.search(v):
        return False
    if PIN6_RE.match(v):
        return False
    # Must contain letters (to avoid pure numeric house-no)
    if not re.search(r'[A-Za-z]', v):
        return False
    # Minimum length guard
    if len(v) < 6:
        return False
    return True

# Validation dispatcher
def validate_by_category(category: str, field_key: str, value: str) -> Tuple[bool, Optional[str]]:
    """
    Returns (is_valid, skip_reason_if_any)
    """
    if category == "ACCOUNT_NUMBER_KEYWORD":
        ok = is_valid_account_number(value)
        return ok, None if ok else "invalid_account_number"
    if category == "CUSTOMER_ID_KEYWORD":
        ok = is_valid_customer_id(value)
        return ok, None if ok else "invalid_customer_id"
    if category == "NAME_KEYWORD":
        ok = looks_like_human_name(value)
        return ok, None if ok else "invalid_name"
    if category == "DOB_KEYWORD":
        ok = is_valid_dob(value)
        return ok, None if ok else "invalid_dob"
    if category == "ADDRESS_KEYWORD":
        ok = is_valid_address(field_key, value)
        return ok, None if ok else "invalid_address"
    return False, "unknown_category"

# Universal skipping on value
def universal_value_rejects(field_key: str, value: str, category: str) -> Optional[str]:
    v = clean_value(value)
    if not v:
        return "empty"
    if v.lower() == "null":
        return "null_value"
    if re.fullmatch(r'[-\*]+', v):
        return "punctuation_only"
    if URL_RE.search(v):
        return "url_value"
    if IPV4_RE.search(v):
        # Address category also disallows IP; others we disallow universally as requested
        return "ip_value"
    return None

# =========================
# SQLite helpers
# =========================

def ensure_db(path: Path) -> sqlite3.Connection:
    path.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(str(path))
    conn.execute("""
        CREATE TABLE IF NOT EXISTS processed_files (
            file_path TEXT PRIMARY KEY,
            mtime REAL,
            size INTEGER,
            processed_at TEXT
        )
    """)
    conn.commit()
    return conn

def file_fingerprint(p: Path) -> Tuple[float, int]:
    st = p.stat()
    return (st.st_mtime, st.st_size)

def is_already_processed(conn: sqlite3.Connection, file_path: str, mtime: float, size: int) -> bool:
    cur = conn.execute("SELECT mtime, size FROM processed_files WHERE file_path = ?", (file_path,))
    row = cur.fetchone()
    if not row:
        return False
    return (abs(row[0] - mtime) < 1e-6) and (row[1] == size)

def mark_processed(conn: sqlite3.Connection, file_path: str, mtime: float, size: int):
    conn.execute("""
        INSERT INTO processed_files(file_path, mtime, size, processed_at)
        VALUES (?, ?, ?, ?)
        ON CONFLICT(file_path) DO UPDATE SET mtime=excluded.mtime, size=excluded.size, processed_at=excluded.processed_at
    """, (file_path, mtime, size, datetime.now().isoformat(timespec='seconds')))
    conn.commit()

# =========================
# I/O: chunk writers
# =========================

@dataclass
class ChunkState:
    out_dir: Path
    chunk_idx: int = 1
    lines_in_chunk: int = 0
    handle: Optional[Any] = None  # file object

    def ensure_handle(self):
        if self.handle is None:
            self.out_dir.mkdir(parents=True, exist_ok=True)
            fn = self.out_dir / f"chunk{self.chunk_idx:04d}.txt"
            self.handle = fn.open("w", encoding="utf-8", errors="ignore")

    def write_line(self, line: str):
        self.ensure_handle()
        self.handle.write(line + "\n")
        self.lines_in_chunk += 1
        if self.lines_in_chunk >= CHUNK_SIZE:
            self.handle.close()
            self.handle = None
            self.chunk_idx += 1
            self.lines_in_chunk = 0

    def close(self):
        if self.handle is not None:
            self.handle.close()
            self.handle = None

# =========================
# Extraction engine
# =========================

def compile_folder_specs() -> Dict[str, Dict]:
    """
    Prepare per-folder extraction specification:
    - category
    - keyword phrases for that category
    - compiled extraction regexes
    """
    folder_specs = {}
    for folder, category in FOLDER_TO_CATEGORY.items():
        phrases = KEYWORD_PHRASES[category]
        token_patterns = [phrase_to_keyword_pattern(p) for p in phrases]
        regexes = build_extraction_regexes(token_patterns)
        folder_specs[folder] = {
            "category": category,
            "phrases": phrases,
            "regexes": regexes
        }
    return folder_specs

def extract_line_matches(line: str, folder_spec: Dict) -> List[Tuple[str, str]]:
    """
    Returns a list of (field_key, value) pairs from this line (unvalidated).
    Deduplicate by KEYWORD_TYPE within the *line* will be done by caller.
    """
    matches: List[Tuple[str, str]] = []
    regexes = folder_spec["regexes"]

    # JSON form: "key" : "value"
    # Returns only the value; we must also capture the key text from the match? The pattern already binds to keyword.
    # For JSON, we can't retrieve actual key token via group—so run a second small search to find which token hit.
    # Simplify: use a broader regex to capture the key too (rebuild here for runtime):
    # We’ll quickly re-run a generic JSON key/value match filtered by phrases.
    # But we already compiled 'json' above to capture only value. Instead, handle explicit key capture separately here:
    # We'll iterate through each phrase variant and test `"phrase": "val"` quickly.
    text = line

    # JSON keys (explicit phrase loop to get the exact matched key token)
    for phrase in folder_spec["phrases"]:
        kp = phrase_to_keyword_pattern(phrase)
        json_key_pat = re.compile(rf'"(?:\s*{kp}\s*)"\s*:\s*"([^"]*)"', re.IGNORECASE)
        for m in json_key_pat.finditer(text):
            field_key = phrase  # use the canonical phrase as field label
            val = m.group(1)
            matches.append((field_key, val))

    # XML tags (compiled supports \1 = same token)
    xml_pat = None
    for name, pat in regexes:
        if name == "xml":
            xml_pat = pat
            break
    if xml_pat:
        for m in xml_pat.finditer(text):
            field_key = m.group(1)
            val = m.group(2)
            matches.append((field_key, val))

    # Brackets
    bracket_pat = None
    for name, pat in regexes:
        if name == "bracket":
            bracket_pat = pat
            break
    if bracket_pat:
        for m in bracket_pat.finditer(text):
            field_key = m.group(1)
            val = m.group(2)
            matches.append((field_key, val))

    # Colon/Equals/Dash
    ced_pat = None
    for name, pat in regexes:
        if name == "ced":
            ced_pat = pat
            break
    if ced_pat:
        for m in ced_pat.finditer(text):
            field_key = m.group(1)
            val = m.group(2)
            matches.append((field_key, val))

    return matches

# =========================
# Worker processing
# =========================

@dataclass
class FileStats:
    lines_scanned: int = 0
    extracted: int = 0
    skipped_invalid: int = 0
    skipped_no_keyword: int = 0

def process_file(file_path: str, folder: str, output_root: str, folder_spec: Dict) -> Dict:
    """
    Returns a dict with:
      - stats: FileStats (as dict)
      - errors: list[str]
      - examples_extracted / examples_skipped : list of (category, input_line, field, value_or_reason)
      - out_lines_count: int  (for chunking aggregation by main process)
      - emitted_lines: List[str]  (we stream write in parent; here we return them)  <-- To reduce memory, we stream to temp and pass back path
    But we must avoid big memory; so we'll write to a temporary per-process file and return path.
    """
    out_tmp_dir = Path(output_root) / ".tmp"
    out_tmp_dir.mkdir(parents=True, exist_ok=True)
    tmp_file = out_tmp_dir / (Path(file_path).name + f".{os.getpid()}.tmp")
    out_f = tmp_file.open("w", encoding="utf-8", errors="ignore")

    stats = FileStats()
    errors: List[str] = []
    examples_extracted: List[Tuple[str, str, str, str]] = []
    examples_skipped: List[Tuple[str, str, str, str]] = []

    category = FOLDER_TO_CATEGORY[folder]

    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            for raw_line in f:
                line = raw_line.rstrip("\n")
                stats.lines_scanned += 1

                # Find all (field_key, value) pairs
                pairs = extract_line_matches(line, folder_spec)

                if not pairs:
                    stats.skipped_no_keyword += 1
                    continue

                # Per-line de-duplication by KEYWORD_TYPE (category), keep ONLY ONE output per line for this folder
                # Within that, we must choose the first valid (field_key, value) pair that passes universal + category checks.
                kept_value: Optional[str] = None
                kept_field: Optional[str] = None
                kept_reason = None

                # Evaluate pairs in order; keep the first valid one
                chosen = False
                for field_key, value in pairs:
                    v = clean_value(value)
                    # Universal rejects
                    ur = universal_value_rejects(field_key, v, category)
                    if ur:
                        if len(examples_skipped) < 8:
                            examples_skipped.append((category, line, field_key, ur))
                        continue

                    # Category validation
                    ok, reason = validate_by_category(category, field_key, v)
                    if not ok:
                        if len(examples_skipped) < 8:
                            examples_skipped.append((category, line, field_key, reason or "invalid"))
                        continue

                    # This pair is acceptable
                    kept_value = v
                    kept_field = field_key
                    chosen = True
                    break

                if not chosen:
                    stats.skipped_invalid += 1
                    continue

                # Emit exactly one line for this file/line (since same-keyword duplicates collapse)
                # Output format: original_log_line ; path ; field ; KEYWORD_TYPE ; extracted_value
                out_line = f"{line} ; {file_path} ; {kept_field} ; {category} ; {kept_value}"
                out_f.write(out_line + "\n")
                stats.extracted += 1

                if len(examples_extracted) < 8:
                    examples_extracted.append((category, line, kept_field, kept_value))

    except Exception as e:
        tb = f"{file_path} :: {type(e).__name__}: {e}"
        errors.append(tb)

    out_f.close()

    return {
        "tmp_path": str(tmp_file),
        "stats": stats.__dict__,
        "errors": errors,
        "examples_extracted": examples_extracted,
        "examples_skipped": examples_skipped,
    }

# =========================
# Summary machinery
# =========================

@dataclass
class FolderAgg:
    lines_scanned: int = 0
    extracted: int = 0
    skipped_invalid: int = 0
    skipped_no_keyword: int = 0

def write_summary(summary_path: Path,
                  input_root: Path,
                  output_root: Path,
                  start_ts: float,
                  folder_aggs: Dict[str, FolderAgg],
                  category_counts_extracted: Dict[str, int],
                  category_counts_skipped: Dict[str, int],
                  total_files_scanned: int,
                  blank_files: int,
                  errors_list: List[str],
                  examples_extracted: Dict[str, List[Tuple[str, str, str]]],
                  examples_skipped: Dict[str, List[Tuple[str, str, str]]]
                  ):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    total_lines_scanned = sum(a.lines_scanned for a in folder_aggs.values())
    total_extracted = sum(a.extracted for a in folder_aggs.values())
    total_skipped_invalid = sum(a.skipped_invalid for a in folder_aggs.values())
    total_skipped_no_keyword = sum(a.skipped_no_keyword for a in folder_aggs.values())

    with open(summary_path, "w", encoding="utf-8") as f:
        f.write("Summary Report - Keyword Extraction\n")
        f.write(f"Run Completed: {now}\n\n")
        f.write(f"Input Folder: {input_root}\n")
        f.write(f"Output Folder: {output_root}\n\n")

        f.write("-------------------------\nOverall Stats\n-------------------------\n")
        f.write(f"Total Files Scanned         : {total_files_scanned}\n")
        f.write(f"Total Lines Scanned         : {total_lines_scanned}\n")
        f.write(f"Total Extracted Lines       : {total_extracted}\n")
        f.write(f"Total Skipped (Invalid)     : {total_skipped_invalid}\n")
        f.write(f"Total Skipped (No Keyword)  : {total_skipped_no_keyword}\n")
        f.write(f"Total Blank Files           : {blank_files}\n")
        f.write(f"Total Errors                : {len(errors_list)}\n\n")

        f.write("-------------------------\nPer-Folder Stats\n-------------------------\n")
        for folder in FOLDER_TO_CATEGORY.keys():
            a = folder_aggs.get(folder, FolderAgg())
            f.write(f"[{folder}/]\n")
            f.write(f"Total Lines Scanned   : {a.lines_scanned}\n")
            f.write(f"Extracted Lines       : {a.extracted}\n")
            f.write(f"Skipped (Invalid)     : {a.skipped_invalid}\n")
            f.write(f"Skipped (No Keyword)  : {a.skipped_no_keyword}\n\n")

        f.write("-------------------------\nPer-Category Stats\n-------------------------\n")
        for cat in FOLDER_TO_CATEGORY.values():
            ext = category_counts_extracted.get(cat, 0)
            skp = category_counts_skipped.get(cat, 0)
            f.write(f"{cat:22s}: {ext} extracted, {skp} skipped\n")
        f.write("\n")

        f.write("-------------------------\nExamples\n-------------------------\n")
        for cat, items in examples_extracted.items():
            if not items:
                continue
            f.write(f"[{cat}] (Extracted)\n")
            for (input_line, field, value) in items[:5]:
                f.write(f"Input Line: {input_line[:200]}\n")
                f.write(f"Extracted : {value} (field={field})\n\n")

        for cat, items in examples_skipped.items():
            if not items:
                continue
            f.write(f"[{cat}] (Skipped)\n")
            for (input_line, field, reason) in items[:5]:
                f.write(f"Input Line: {input_line[:200]}\n")
                f.write(f"Skipped   : {reason} (field={field})\n\n")

        f.write("-------------------------\nErrors\n-------------------------\n")
        if not errors_list:
            f.write("(No errors logged)\n")
        else:
            for e in errors_list[:200]:
                f.write(e + "\n")

# =========================
# Main
# =========================

def find_txt_files(root: Path, recursive: bool = True) -> List[Path]:
    out = []
    for p in root.iterdir():
        if p.is_file() and p.suffix.lower() in ALLOWED_EXTS:
            out.append(p)
    # If recursive inside subfolders (we expect plain files in each category folder)
    for d in root.iterdir():
        if d.is_dir():
            for p in d.rglob("*.txt"):
                out.append(p)
    # But we only want files directly under each category folder, not across categories.
    # The caller will enumerate per-folder explicitly.
    return out

def enumerate_folder_files(input_root: Path) -> Dict[str, List[Path]]:
    """
    Return mapping folder_name -> list of .txt files within that subfolder (non-recursive).
    """
    result: Dict[str, List[Path]] = {}
    for folder in FOLDER_TO_CATEGORY.keys():
        sub = input_root / folder
        if not sub.exists() or not sub.is_dir():
            result[folder] = []
            continue
        files = sorted([p for p in sub.iterdir() if p.is_file() and p.suffix.lower() in ALLOWED_EXTS])
        result[folder] = files
    return result

def append_error(error_path: Path, msg: str):
    with open(error_path, "a", encoding="utf-8") as ef:
        ef.write(f"{datetime.now().isoformat(timespec='seconds')} {msg}\n")

def main():
    parser = argparse.ArgumentParser(
        description="Keyword-based extraction with ProcessPool, SQLite resume, chunked outputs, and comprehensive summary."
    )
    parser.add_argument("--input", required=True, help="Input root folder containing subfolders: account_number, customer_id, name, dob, address")
    parser.add_argument("--output", required=True, help="Output root folder (mirrors input subfolders)")
    parser.add_argument("--workers", type=int, default=DEFAULT_MAX_WORKERS, help="Number of worker processes (default: 6)")
    parser.add_argument("--force", action="store_true", help="Reprocess all files even if unchanged (ignore SQLite resume)")
    args = parser.parse_args()

    input_root = Path(args.input).resolve()
    output_root = Path(args.output).resolve()
    output_root.mkdir(parents=True, exist_ok=True)

    db_path = output_root / DB_NAME
    error_path = output_root / ERROR_LOG
    summary_path = output_root / SUMMARY_FILENAME
    resume_log_path = output_root / RESUME_LOG

    conn = ensure_db(db_path)

    folder_specs = compile_folder_specs()
    folder_files = enumerate_folder_files(input_root)

    # Aggregations
    folder_aggs: Dict[str, FolderAgg] = {k: FolderAgg() for k in FOLDER_TO_CATEGORY.keys()}
    category_counts_extracted: Dict[str, int] = Counter()
    category_counts_skipped: Dict[str, int] = Counter()
    examples_extracted: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)  # cat -> [(input_line, field, value)]
    examples_skipped: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)    # cat -> [(input_line, field, reason)]
    errors_list: List[str] = []
    blank_files = 0

    # For output chunking states per folder
    chunk_states: Dict[str, ChunkState] = {}
    for folder in FOLDER_TO_CATEGORY.keys():
        out_dir = output_root / folder
        chunk_states[folder] = ChunkState(out_dir=out_dir)

    # Live summary refresher
    lock = threading.Lock()
    start_ts = time.time()
    total_files_scanned = 0

    def refresher():
        while not stop_event.is_set():
            with lock:
                try:
                    write_summary(summary_path, input_root, output_root, start_ts,
                                  folder_aggs, category_counts_extracted, category_counts_skipped,
                                  total_files_scanned, blank_files, errors_list,
                                  examples_extracted, examples_skipped)
                except Exception as e:
                    append_error(error_path, f"[summary_refresh] {type(e).__name__}: {e}")
            stop_event.wait(30.0)

    stop_event = threading.Event()
    t = threading.Thread(target=refresher, daemon=True)
    t.start()

    # Process per folder
    try:
        for folder, files in folder_files.items():
            spec = folder_specs[folder]
            category = spec["category"]
            out_state = chunk_states[folder]

            if not files:
                continue

            with concurrent.futures.ProcessPoolExecutor(max_workers=args.workers) as ex:
                futures = []
                # Submit only files that need processing
                for p in files:
                    try:
                        mtime, size = file_fingerprint(p)
                        if (not args.force) and is_already_processed(conn, str(p), mtime, size):
                            total_files_scanned += 1
                            # Still count blank files if zero-sized
                            if size == 0:
                                blank_files += 1
                            continue
                        futures.append(ex.submit(process_file, str(p), folder, str(output_root), spec))
                    except Exception as e:
                        msg = f"[submit] {p}: {type(e).__name__}: {e}"
                        errors_list.append(msg)
                        append_error(error_path, msg)

                for fut in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=f"{folder}"):
                    try:
                        res = fut.result()
                    except Exception as e:
                        msg = f"[future] {type(e).__name__}: {e}"
                        errors_list.append(msg)
                        append_error(error_path, msg)
                        continue

                    # Aggregate stats
                    stats = res["stats"]
                    folder_aggs[folder].lines_scanned += stats["lines_scanned"]
                    folder_aggs[folder].extracted += stats["extracted"]
                    folder_aggs[folder].skipped_invalid += stats["skipped_invalid"]
                    folder_aggs[folder].skipped_no_keyword += stats["skipped_no_keyword"]
                    total_files_scanned += 1
                    if stats["lines_scanned"] == 0:
                        blank_files += 1

                    # Examples
                    for (cat, input_line, field, value) in res["examples_extracted"]:
                        if len(examples_extracted[cat]) < 5:
                            examples_extracted[cat].append((input_line, field, value))
                    for (cat, input_line, field, reason) in res["examples_skipped"]:
                        if len(examples_skipped[cat]) < 5:
                            examples_skipped[cat].append((input_line, field, reason))

                    # For per-category counters:
                    category_counts_extracted[category] += stats["extracted"]
                    category_counts_skipped[category] += stats["skipped_invalid"]

                    # Stream the tmp output into proper chunked files
                    tmp_path = Path(res["tmp_path"])
                    try:
                        if tmp_path.exists():
                            with tmp_path.open("r", encoding="utf-8", errors="ignore") as tf:
                                for out_line in tf:
                                    out_state.write_line(out_line.rstrip("\n"))
                            tmp_path.unlink(missing_ok=True)
                    except Exception as e:
                        msg = f"[merge_tmp] {tmp_path}: {type(e).__name__}: {e}"
                        errors_list.append(msg)
                        append_error(error_path, msg)

                # Mark processed in DB after folder batch completes (or per file if you prefer tighter resume granularity)
                for p in files:
                    try:
                        mtime, size = file_fingerprint(p)
                        if args.force or (not is_already_processed(conn, str(p), mtime, size)):
                            mark_processed(conn, str(p), mtime, size)
                            with open(resume_log_path, "a", encoding="utf-8") as rlog:
                                rlog.write(f"{datetime.now().isoformat(timespec='seconds')} PROCESSED {p}\n")
                    except Exception as e:
                        msg = f"[mark_processed] {p}: {type(e).__name__}: {e}"
                        errors_list.append(msg)
                        append_error(error_path, msg)

            # Close chunk file handle for this folder
            out_state.close()

    finally:
        stop_event.set()
        t.join(timeout=2.0)

    # Final summary write
    with lock:
        write_summary(summary_path, input_root, output_root, start_ts,
                      folder_aggs, category_counts_extracted, category_counts_skipped,
                      total_files_scanned, blank_files, errors_list,
                      examples_extracted, examples_skipped)

    print(f"\nDone. Summary written to: {summary_path}")
    print(f"DB at: {db_path}")
    print(f"Errors (if any) at: {error_path}")

if __name__ == "__main__":
    main()
