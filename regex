import os
import re
import sys
import time
import gzip
import traceback
from collections import Counter
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import lru_cache
from datetime import datetime
from tqdm import tqdm

# ====== CONFIGURATION ====== #
INPUT_FOLDER = r"C:\Users\manav\Documents\pii_logs_gz"  # ðŸ”§ Folder with .gz inputs (non-recursive)
OUTPUT_FOLDER = "output_regex"                           # Outputs as .gz (same basenames)
SUMMARY_FILE = "summary_report.txt"                      # Saved in current working dir
RESUME_LOG   = "resume_files.log"                        # âœ… Checkpoint log in current working dir
MAX_WORKERS  = 6                                         # Use 6â€“8
SUMMARY_EVERY_SECS = 30
GZIP_LEVEL   = 1                                         # Fast gzip for logs

# Long-line EMAIL/UPI handling
ENABLE_EMAIL_WINDOWING = True
ENABLE_UPI_WINDOWING   = True
# You asked to NOT skip on very long lines:
ENABLE_EMAIL_HARD_SKIP = False
ENABLE_UPI_HARD_SKIP   = False

LONG_LINE_WINDOW = 50_000         # > this: use windowing around '@'
HARD_SKIP_LEN    = 1_000_000      # >= this: (disabled by flags above)
EMAIL_WIN_L, EMAIL_WIN_R = 256, 320
UPI_WIN_L,   UPI_WIN_R   = 256,  64
MAX_ATS_PER_LINE = 2000
# =========================== #

# ---------- REGEX PATTERNS ----------
PII_PATTERNS = {
    "MOBILE_REGEX": re.compile(r'(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])'),
    "AADHAAR_REGEX": re.compile(r'(?<![A-Za-z0-9])(\d{12})(?![A-Za-z0-9])'),
    "PAN_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{5}\d{4}[A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "GSTIN_REGEX": re.compile(r'(?<![A-Za-z0-9])\d{2}[A-Z]{5}\d{4}[A-Z][1-9A-Z]Z[0-9A-Z](?![A-Za-z0-9])', re.IGNORECASE),
    "DL_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{2}\d{2}\d{11}(?![A-Za-z0-9])', re.IGNORECASE),
    "VOTERID_REGEX": re.compile(r'(?<![A-Za-z0-9])[A-Z]{3}\d{7}(?![A-Za-z0-9])', re.IGNORECASE),

    # EMAIL: TLD >=2 (e.g., .co, .com, .net, .org, .in, .co.in via the domain part)
    "EMAIL_REGEX": re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', re.IGNORECASE),

    # UPI: block email-like continuations (.com etc.); allow EOL or delimiters after PSP.
    # Allowed delimiters after PSP: whitespace, < > [ ] { } ( ) , ; _ -
    "UPI_REGEX": re.compile(
        r'[A-Za-z0-9._-]{2,256}@[A-Za-z]{2,64}(?=$|[\s<>\[\]\{\}\(\),;_\-])'
    ),

    # IPv4 strict 0â€“255
    "IP_REGEX": re.compile(
        r'(?<!\d)'
        r'(?:(?:25[0-5]|2[0-4]\d|1?\d?\d)\.){3}'
        r'(?:25[0-5]|2[0-4]\d|1?\d?\d)'
        r'(?!\d)'
    ),

    "MAC_REGEX": re.compile(r'(?<![A-Fa-f0-9])(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}(?![A-Fa-f0-9])'),

    # Card numbers with alnum lookarounds; validated with Luhn.
    "CARD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])('
        r'4\d{12}(?:\d{3})?'                                 # Visa
        r'|5[1-5]\d{14}'                                     # Mastercard (old range)
        r'|2(?:2[2-9]\d{12}|[3-6]\d{13}|7(?:[01]\d{12}|20\d{12}))'  # Mastercard 2-series
        r'|3[47]\d{13}'                                      # Amex
        r'|60\d{14}|65\d{14}|81\d{14}|508\d\d{12}'           # Other BINs
        r')(?![A-Za-z0-9])'
    ),

    # Coords: decimals required for both lat/lon
    "COORD_REGEX": re.compile(
        r'(?<![A-Za-z0-9])'
        r'([+-]?(?:90\.(?:0+)?|[0-8]?\d\.\d+))'
        r'\s*,\s*'
        r'([+-]?(?:180\.(?:0+)?|1[0-7]\d\.\d+|[0-9]?\d\.\d+))'
        r'(?![A-Za-z0-9])'
    ),
}

# ---------- SUMMARY (aggregated in parent) ----------
summary = {
    "files_scanned": 0,
    "files_success": 0,
    "files_error": 0,
    "blank_input_files": [],      # input had 0 lines
    "zero_match_files": [],       # had lines but wrote none
    "errors": [],                 # list of error strings

    # counters
    "total_lines": 0,             # input lines scanned
    "output_lines_written": 0,    # rows written (one per unique (value,type))
    "total_no_match_lines": 0,    # sum of per-file no_match_lines
    "per_type_counts": Counter({k: 0 for k in PII_PATTERNS}),
    "total_matches": 0,           # sum(per_type_counts)

    # long-line telemetry
    "email_windowed_lines": 0,
    "email_too_long_lines": 0,    # will remain 0 (hard-skip disabled)
    "upi_windowed_lines": 0,
    "upi_too_long_lines": 0,      # will remain 0 (hard-skip disabled)

    # timing/config
    "start_ts": None,
    "end_ts": None,
    "max_workers": MAX_WORKERS,
}

# ---------- Aadhaar Verhoeff ----------
@lru_cache(maxsize=10000)
def is_valid_aadhaar(number: str) -> bool:
    if len(number) != 12 or not number.isdigit():
        return False
    mul = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,2,3,4,0,6,7,8,9,5],
        [2,3,4,0,1,7,8,9,5,6],
        [3,4,0,1,2,8,9,5,6,7],
        [4,0,1,2,3,9,5,6,7,8],
        [5,9,8,7,6,0,4,3,2,1],
        [6,5,9,8,7,1,0,4,3,2],
        [7,6,5,9,8,2,1,0,4,3],
        [8,7,6,5,9,3,2,1,0,4],
        [9,8,7,6,5,4,3,2,1,0]
    ]
    perm = [
        [0,1,2,3,4,5,6,7,8,9],
        [1,5,7,6,2,8,3,0,9,4],
        [5,8,0,3,7,9,6,1,4,2],
        [8,9,1,6,0,4,3,5,2,7],
        [9,4,5,3,1,2,6,8,7,0],
        [4,2,8,6,5,7,3,9,0,1],
        [2,7,9,3,8,0,6,4,1,5],
        [7,0,4,6,9,1,3,2,5,8]
    ]
    inv = [0,4,3,2,1,5,6,7,8,9]
    c = 0
    for i, ch in enumerate(reversed(number)):
        c = mul[c][perm[i % 8][int(ch)]]
    return inv[c] == 0

# ---------- Card Luhn ----------
def _luhn_valid(num: str) -> bool:
    s, alt = 0, False
    for ch in reversed(num):
        if not ch.isdigit():
            return False
        n = ord(ch) - 48
        if alt:
            n *= 2
            if n > 9:
                n -= 9
        s += n
        alt = not alt
    return (s % 10) == 0

# ---------- EMAIL/UPI windowing helpers ----------
def _windows_around_at(line: str, left: int, right: int, max_ats: int):
    """Yield (start_index_in_line, window_text) around each '@' (up to max_ats)."""
    idxs = [i for i, ch in enumerate(line) if ch == '@']
    if len(idxs) > max_ats:
        idxs = idxs[:max_ats]
    L = len(line)
    for i in idxs:
        start = max(0, i - left)
        end   = min(L, i + 1 + right)
        yield start, line[start:end]

# ---------- Core matching (dedup per line) ----------
def extract_matches(log_line: str):
    """
    Returns:
      unique_matches: set of (value, pii_type)
      email_mode: 'FULL' | 'WINDOWED'
      upi_mode:   'FULL' | 'WINDOWED'
    Notes:
      - Aadhaar validated by Verhoeff; cards by Luhn
      - UPI candidates overlapping EMAIL spans are dropped
    """
    unique = set()

    # ---- EMAIL first (collect spans for UPI shielding) ----
    email_spans = []
    if ENABLE_EMAIL_WINDOWING and len(log_line) > LONG_LINE_WINDOW:
        email_mode = "WINDOWED"
        for start, chunk in _windows_around_at(log_line, EMAIL_WIN_L, EMAIL_WIN_R, MAX_ATS_PER_LINE):
            for m in PII_PATTERNS["EMAIL_REGEX"].finditer(chunk):
                s, e = start + m.start(), start + m.end()
                email_spans.append((s, e))
                unique.add((m.group(0), "EMAIL_REGEX"))
    else:
        # HARD skip disabled: even if very long, we don't skip; we scan FULL
        email_mode = "FULL"
        for m in PII_PATTERNS["EMAIL_REGEX"].finditer(log_line):
            email_spans.append((m.start(), m.end()))
            unique.add((m.group(0), "EMAIL_REGEX"))

    # ---- UPI with right-lookahead + shield from emails ----
    if ENABLE_UPI_WINDOWING and len(log_line) > LONG_LINE_WINDOW:
        upi_mode = "WINDOWED"
        for start, chunk in _windows_around_at(log_line, UPI_WIN_L, UPI_WIN_R, MAX_ATS_PER_LINE):
            for m in PII_PATTERNS["UPI_REGEX"].finditer(chunk):
                s, e = start + m.start(), start + m.end()
                # drop if overlaps an email span
                overlap = any(not (e <= es or s >= ee) for es, ee in email_spans)
                if overlap:
                    continue
                unique.add((m.group(0), "UPI_REGEX"))
    else:
        upi_mode = "FULL"
        for m in PII_PATTERNS["UPI_REGEX"].finditer(log_line):
            s, e = m.start(), m.end()
            overlap = any(not (e <= es or s >= ee) for es, ee in email_spans)
            if overlap:
                continue
            unique.add((m.group(0), "UPI_REGEX"))

    # ---- OTHER PATTERNS (full line) ----
    for pii_type, pattern in PII_PATTERNS.items():
        if pii_type in ("EMAIL_REGEX", "UPI_REGEX"):
            continue
        if not pattern.search(log_line):
            continue
        for m in pattern.finditer(log_line):
            value = m.group(0)
            if pii_type == "AADHAAR_REGEX" and not is_valid_aadhaar(value):
                continue
            if pii_type == "CARD_REGEX" and not _luhn_valid(value):
                continue
            unique.add((value, pii_type))

    return unique, email_mode, upi_mode

def process_file(file_path: str) -> dict:
    """
    Runs in a separate process. Streams .gz -> .gz, no temp files.
    Output format: log_line ; path ; value ; pii_type
    """
    local = {
        "file_name": os.path.basename(file_path),
        "lines": 0,
        "no_match_lines": 0,
        "written": 0,
        "per_type_counts": Counter({k: 0 for k in PII_PATTERNS}),
        "error": None,
        "input_was_blank": False,
        # long-line telemetry
        "email_windowed_lines": 0,
        "email_too_long_lines": 0,  # stays 0 with hard-skip disabled
        "upi_windowed_lines": 0,
        "upi_too_long_lines": 0,    # stays 0 with hard-skip disabled
    }
    out_path = os.path.join(OUTPUT_FOLDER, os.path.basename(file_path))  # output .gz

    # Clean any stale partial from a previous failed attempt
    try:
        if os.path.exists(out_path):
            os.remove(out_path)
    except Exception:
        pass

    try:
        with gzip.open(file_path, "rt", encoding="utf-8", errors="ignore") as f_in, \
             gzip.open(out_path,  "wt", encoding="utf-8", compresslevel=GZIP_LEVEL) as f_out:

            any_line = False
            for raw in f_in:
                any_line = True
                local["lines"] += 1
                line = raw.rstrip("\n")

                if ";" in line:
                    log_line, path = line.rsplit(";", 1)
                    log_line = log_line.rstrip()
                    path = path.strip()
                else:
                    log_line, path = line, "UNKNOWN_PATH"

                unique_matches, email_mode, upi_mode = extract_matches(log_line)

                # telemetry
                if email_mode == "WINDOWED": local["email_windowed_lines"] += 1
                if upi_mode   == "WINDOWED": local["upi_windowed_lines"]   += 1

                if unique_matches:
                    for value, pii_type in sorted(unique_matches):
                        f_out.write(f"{log_line} ; {path} ;{value};{pii_type}\n")
                        local["written"] += 1
                        local["per_type_counts"][pii_type] += 1
                else:
                    local["no_match_lines"] += 1

            if not any_line:
                local["input_was_blank"] = True
                # empty output file created

    except Exception as e:
        # Remove partial output so the file is retried next run
        try:
            if os.path.exists(out_path):
                os.remove(out_path)
        except Exception:
            pass
        err = f"{local['file_name']}: {e.__class__.__name__}: {e}"
        if isinstance(e, OSError) and getattr(e, 'errno', None) is not None:
            err += f" (errno={e.errno})"
        err += "\n" + "".join(traceback.format_exception_only(type(e), e)).strip()
        local["error"] = err

    return local

# ---------- Resume helpers ----------
def load_completed_set(log_path: str) -> set:
    completed = set()
    if os.path.exists(log_path):
        with open(log_path, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                name = line.strip()
                if name and not line.startswith("#"):
                    completed.add(name)
    return completed

def append_completed(log_path: str, file_name: str):
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(file_name + "\n")
        f.flush()

# ---------- Summary writer ----------
def write_summary():
    summary["end_ts"] = time.time()
    summary["total_matches"] = sum(summary["per_type_counts"].values())

    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write(f"Summary Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Input Folder: {os.path.abspath(INPUT_FOLDER)}\n")
        f.write(f"Output Folder: {os.path.abspath(OUTPUT_FOLDER)}\n")
        f.write(f"Max Workers: {summary['max_workers']}\n")
        f.write(f"GZIP Level: {GZIP_LEVEL}\n")
        f.write(f"EMAIL windowing: {ENABLE_EMAIL_WINDOWING}; hard-skip: {ENABLE_EMAIL_HARD_SKIP}\n")
        f.write(f"UPI   windowing: {ENABLE_UPI_WINDOWING}; hard-skip: {ENABLE_UPI_HARD_SKIP}\n\n")

        # ---- Files processed ----
        f.write("=== Files Processed ===\n")
        f.write(f"Processed: {summary['files_scanned']}\n")
        f.write(f"Success:   {summary['files_success']}\n")
        f.write(f"Errors:    {summary['files_error']}\n")
        f.write(f"Blank input files: {len(summary['blank_input_files'])}\n")
        f.write(f"Files with 0 matches: {len(summary['zero_match_files'])}\n\n")

        # ---- Line counts ----
        f.write("=== Line Counts (this run) ===\n")
        f.write(f"Total lines scanned: {summary['total_lines']}\n")
        f.write(f"Lines written:       {summary['output_lines_written']}\n")
        f.write(f"Lines removed:       {summary['total_no_match_lines']}\n\n")

        # ---- PII type counts ----
        f.write("=== PII Type Counts (this run) ===\n")
        for pii_type, count in sorted(summary['per_type_counts'].items()):
            f.write(f"- {pii_type}: {count}\n")
        f.write(f"(Sanity: sum(per-type)={summary['total_matches']} vs lines written={summary['output_lines_written']})\n\n")

        # ---- Files with 0 matches ----
        if summary['zero_match_files']:
            f.write("=== Files With 0 Matches ===\n")
            for fname in sorted(summary['zero_match_files']):
                f.write(f"- {fname}\n")
            f.write("\n")

        # ---- Blank files ----
        if summary['blank_input_files']:
            f.write("=== Blank Input Files (0 lines) ===\n")
            for fname in sorted(summary['blank_input_files']):
                f.write(f"- {fname}\n")
            f.write("\n")

        # ---- Errors ----
        if summary['errors']:
            f.write("=== Errors (this run) ===\n")
            for err in summary['errors']:
                f.write(f"- {err}\n")

# ---------- Main ----------
def main():
    if not os.path.isdir(INPUT_FOLDER):
        print(f"ERROR: INPUT_FOLDER does not exist: {INPUT_FOLDER}", file=sys.stderr)
        sys.exit(1)

    summary["start_ts"] = time.time()
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)

    # Discover inputs (.gz only, stable order)
    all_files = sorted(
        os.path.join(INPUT_FOLDER, f)
        for f in os.listdir(INPUT_FOLDER)
        if f.endswith(".gz") and os.path.isfile(os.path.join(INPUT_FOLDER, f))
    )

    if not all_files:
        print("No .gz files found in INPUT_FOLDER.", file=sys.stderr)
        write_summary()  # write empty summary shell
        sys.exit(2)

    # Resume set: ONLY resume log (do NOT infer from existing outputs)
    completed = load_completed_set(RESUME_LOG)
    pending_files = [fp for fp in all_files if os.path.basename(fp) not in completed]

    if not pending_files:
        print("All files already processed per resume log. Nothing to do.")
        write_summary()
        return

    overall_bar = tqdm(total=len(pending_files), desc="Overall", unit="file", leave=True)
    last_summary_write = time.time()

    try:
        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:
            futures = {ex.submit(process_file, fp): fp for fp in pending_files}

            for fut in as_completed(futures):
                src = futures[fut]
                base = os.path.basename(src)

                try:
                    local = fut.result()
                except Exception as e:
                    summary["files_scanned"] += 1
                    summary["files_error"] += 1
                    summary["errors"].append(f"{base}: worker exception: {e}")
                    overall_bar.update(1)
                    if time.time() - last_summary_write >= SUMMARY_EVERY_SECS:
                        write_summary()
                        last_summary_write = time.time()
                    continue

                summary["files_scanned"] += 1
                summary["total_lines"] += local["lines"]
                summary["output_lines_written"] += local["written"]
                summary["total_no_match_lines"] += local["no_match_lines"]  # âœ… true removed-lines count
                summary["per_type_counts"].update(local["per_type_counts"])

                # long-line telemetry
                summary["email_windowed_lines"] += local["email_windowed_lines"]
                summary["email_too_long_lines"] += local["email_too_long_lines"]  # stays 0
                summary["upi_windowed_lines"]   += local["upi_windowed_lines"]
                summary["upi_too_long_lines"]   += local["upi_too_long_lines"]    # stays 0

                if local["input_was_blank"]:
                    summary["blank_input_files"].append(local["file_name"])
                if local["lines"] > 0 and local["written"] == 0:
                    summary["zero_match_files"].append(local["file_name"])

                if local["error"]:
                    summary["files_error"] += 1
                    summary["errors"].append(local["error"])
                else:
                    summary["files_success"] += 1
                    append_completed(RESUME_LOG, base)

                overall_bar.update(1)

                # Periodic summary flush
                if time.time() - last_summary_write >= SUMMARY_EVERY_SECS:
                    write_summary()
                    last_summary_write = time.time()

    finally:
        overall_bar.close()
        write_summary()

if __name__ == "__main__":
    main()
