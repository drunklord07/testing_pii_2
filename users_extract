#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import sys
import traceback
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from collections import Counter, defaultdict
from datetime import datetime
from tqdm import tqdm

# ========== CONFIGURATION ==========
INPUT_FOLDER = "input_logs"          # <-- Hardcode your folder here
OUTPUT_FOLDER = "output_users"       # Combined extracted matches
NEW_ORIGINAL_FOLDER = "new_original" # Mirror of input minus extracted lines
SUMMARY_FILE = "summary_users.txt"
CHUNK_SIZE = 10000
MAX_WORKERS = 6
ALLOWED_EXTS = (".txt",)

# Mobile number detection: 10â€“12 digits, not embedded inside larger alphanumerics
MOBILE_REGEX = r"(?<![A-Za-z0-9])(?:91)?[6-9]\d{9}(?![A-Za-z0-9])"

# Keywords list (case-insensitive, flexible spacing)
KEYWORDS = [
    "mobile number", "mobile no", "mobileno",
    "cust id", "customer id", "custid",
    "phone number",
    "account number", "account no", "account",
    "cust", "customerid"
]
# ===================================

# Precompile regexes for all case types
PATTERNS = {
    "bracket": re.compile(r"\b(\w+)\s*=\s*\[\s*(" + MOBILE_REGEX + r")\s*\]", re.IGNORECASE),
    "coloncolon": re.compile(r"\b([\w\s]+?)\s*::\s*(" + MOBILE_REGEX + r")", re.IGNORECASE),
    "hyphen": re.compile(r"\b([\w\s]+?)\s*-\s*(" + MOBILE_REGEX + r")", re.IGNORECASE),
    "json": re.compile(r"[\"']\s*([\w\s]+?)\s*[\"']\s*:\s*[\"']?\s*(" + MOBILE_REGEX + r")\s*[\"']?", re.IGNORECASE),
    # keyword pattern built dynamically
}
KEYWORD_PATTERN = re.compile(
    r"\b(" + "|".join(re.escape(k) for k in KEYWORDS) + r")\b\s*(" + MOBILE_REGEX + r")",
    re.IGNORECASE
)

def find_txt_files(root: Path):
    return sorted([p for p in root.rglob("*") if p.suffix in ALLOWED_EXTS])

def normalize_field(field: str) -> str:
    """Clean up and normalize field name (trim, collapse spaces)."""
    return re.sub(r"\s+", " ", field.strip())

def extract_from_logline(logline: str):
    """
    Scan a single log line for all known mobile extraction patterns.
    Returns (formatted_line, number, field, case_type) or None.
    """
    # Case 1: bracket style users=[...]
    m = PATTERNS["bracket"].search(logline)
    if m:
        field, number = normalize_field(m.group(1)), m.group(2)
        formatted = f"{field}=[{number}]"
        return formatted, number, field, "bracket"

    # Case 2: colon-colon style
    m = PATTERNS["coloncolon"].search(logline)
    if m:
        field, number = normalize_field(m.group(1)), m.group(2)
        formatted = f"{field}=[{number}]"
        return formatted, number, field, "coloncolon"

    # Case 3: hyphen style
    m = PATTERNS["hyphen"].search(logline)
    if m:
        field, number = normalize_field(m.group(1)), m.group(2)
        formatted = f"{field}=[{number}]"
        return formatted, number, field, "hyphen"

    # Case 4: JSON-like style (with or without escaped quotes)
    m = PATTERNS["json"].search(logline)
    if m:
        field, number = normalize_field(m.group(1)), m.group(2)
        formatted = f"{field}=[{number}]"
        return formatted, number, field, "json"

    # Case 5: keyword style
    m = KEYWORD_PATTERN.search(logline)
    if m:
        field, number = normalize_field(m.group(1)), m.group(2)
        formatted = f"{field}=[{number}]"
        return formatted, number, field, "keyword"

    return None

def process_file(file_path: Path, input_root: Path):
    stats = {
        "lines_scanned": 0,
        "lines_extracted": 0,
        "lines_remaining": 0,
        "numbers": Counter(),
        "case_types": Counter(),
        "error": None,
        "blank": False
    }
    extracted = []
    cleaned_lines = []
    try:
        with file_path.open("r", encoding="utf-8", errors="ignore") as f:
            for raw in f:
                line = raw.rstrip("\n")
                stats["lines_scanned"] += 1

                # split from right for path etc.
                parts = [p.strip() for p in line.rsplit(";", 4)]
                if len(parts) != 5:
                    cleaned_lines.append(line)
                    continue
                logline, path_in_line, _unid, _regex_type, _mobile_match = parts

                res = extract_from_logline(logline)
                if res:
                    formatted, number, field, case_type = res
                    outline = f"{formatted} ; {path_in_line} ; {field} ; MOBILE_REGEX ; {number}"
                    extracted.append(outline)
                    stats["lines_extracted"] += 1
                    stats["numbers"][number] += 1
                    stats["case_types"][case_type] += 1
                else:
                    cleaned_lines.append(line)

        if stats["lines_scanned"] == 0:
            stats["blank"] = True
        stats["lines_remaining"] = len(cleaned_lines)
    except Exception as e:
        stats["error"] = f"{file_path}: {e}"

    # Write cleaned file into new_original/
    rel_path = file_path.relative_to(input_root)
    new_path = Path(NEW_ORIGINAL_FOLDER) / rel_path
    new_path.parent.mkdir(parents=True, exist_ok=True)
    with new_path.open("w", encoding="utf-8") as out_clean:
        for l in cleaned_lines:
            out_clean.write(l + "\n")

    return file_path, extracted, stats

def write_combined_chunks(all_outputs, out_dir):
    out_dir.mkdir(parents=True, exist_ok=True)
    chunk_idx = 1
    line_count = 0
    writer = None

    for line in all_outputs:
        if writer is None or line_count >= CHUNK_SIZE:
            if writer:
                writer.close()
            chunk_file = out_dir / f"combined_chunk{chunk_idx:04d}.txt"
            writer = chunk_file.open("w", encoding="utf-8")
            chunk_idx += 1
            line_count = 0
        writer.write(line + "\n")
        line_count += 1
    if writer:
        writer.close()

def write_summary(summary, numbers_counter, case_counter):
    with open(SUMMARY_FILE, "w", encoding="utf-8") as f:
        f.write("Summary Report - Mobile Extractor\n")
        f.write(f"Generated: {datetime.now()}\n\n")
        f.write(f"Input folder        : {summary['input']}\n")
        f.write(f"Output folder       : {OUTPUT_FOLDER}\n")
        f.write(f"New original folder : {NEW_ORIGINAL_FOLDER}\n")
        f.write(f"Files scanned       : {summary['files_scanned']}\n")
        f.write(f"Total lines scanned : {summary['lines_scanned']}\n")
        f.write(f"Total extracted     : {summary['lines_extracted']}\n")
        f.write(f"Total new_original  : {summary['lines_remaining']}\n")
        f.write(f"Blank files         : {summary['blank_files']}\n")
        f.write(f"Errors              : {len(summary['errors'])}\n")
        if summary["errors"]:
            f.write("Error details:\n")
            for e in summary["errors"]:
                f.write(f"  - {e}\n")

        f.write("\nPer-case breakdown:\n")
        for case, count in case_counter.items():
            f.write(f"  {case} : {count}\n")

        f.write("\nPer-number frequency:\n")
        for num, count in numbers_counter.most_common():
            f.write(f"  {num} : {count}\n")

def main():
    input_folder = Path(INPUT_FOLDER)
    if not input_folder.exists():
        print(f"Input folder not found: {input_folder}")
        sys.exit(1)

    txt_files = find_txt_files(input_folder)
    summary = {
        "input": str(input_folder),
        "files_scanned": 0,
        "lines_scanned": 0,
        "lines_extracted": 0,
        "lines_remaining": 0,
        "blank_files": 0,
        "errors": []
    }
    numbers_counter = Counter()
    case_counter = Counter()
    all_extracted = []

    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = {ex.submit(process_file, f, input_folder): f for f in txt_files}
        for fut in tqdm(as_completed(futures), total=len(futures), desc="Processing files"):
            fpath, outputs, stats = fut.result()
            summary["files_scanned"] += 1
            summary["lines_scanned"] += stats["lines_scanned"]
            summary["lines_extracted"] += stats["lines_extracted"]
            summary["lines_remaining"] += stats["lines_remaining"]
            if stats["blank"]:
                summary["blank_files"] += 1
            if stats["error"]:
                summary["errors"].append(stats["error"])
            numbers_counter.update(stats["numbers"])
            case_counter.update(stats["case_types"])
            all_extracted.extend(outputs)

    if all_extracted:
        write_combined_chunks(all_extracted, Path(OUTPUT_FOLDER))

    write_summary(summary, numbers_counter, case_counter)
    print(f"\nDone. Extracted lines in {OUTPUT_FOLDER}, cleaned files in {NEW_ORIGINAL_FOLDER}, summary in {SUMMARY_FILE}")

if __name__ == "__main__":
    main()
